{
  
    
        "post0": {
            "title": "Weight Initialization",
            "content": "In this article, I am writing some notes about weight initialization. I took these from various sources I am reading regarding this. Links and References at the end of the post! Consider reading them to get a clear understanding! . NOTE: This is post for my future self to look back and review the material. So, this’ll be very unpolished! . Introduction . The first step that comes in consideration while building a neural network is the initialization of parameters - weights and biases. If not done correctly then layer activations might explode or vanish during the forward propagation which in turn makes loss gradients to be either too large or too small. Then achieving optimization will take longer or sometimes converging to a minima using gradient descent will be impossible. . Some key points to remember . If the weights are initialized too large or too small, the network won’t learn well - because it leads to exploding or vanishing gradients problem. | All weights should not be initialized with zeros. If neurons starts with same weights, then all neurons will learn the same features and perform the same thing as one another. | Neural Networks try to reach the local minima, If all the weights start at zero - it is not possible. So, it is better to give them different starting values. | . | . Weight Initialization methods . Normal Initialization . The authors of the famous Alexnet Paper initialized weights using zero-mean Guassian (normal) distribution with a standard deviation of 0.01. The biases were initialized as 1 for some layers and 0 for the rest. . Uniform initialization: bounded uniformly between ~ [$ frac{-1}{ sqrt{f_{in}}}, frac{1}{ sqrt{f_{in}}}$] . But this normal random initialization of weights does not work well for training deep neural networks, because of vanishing and exploding gradient problem. . Xavier Initialization / Glorot initialization [paper] . Proposed by Xavier and Bengio | considers number of input and output units while initializing weights | weights stay within a reasonable range by making them inversely proportional to the square root of the number of units in the previous layer | . Uniform: bounded uniformly between ~ [$ pm sqrt { frac {6} {f_{in} + f_{out}}}$] . Normal: multiply normal distribution by $ sqrt { frac {2} {f_{in} + f_{out}}}$ . np.random.rand(shape) * np.sqrt( $ frac {2} {f_{in} + f_{out}}$) | or create normal distribution with $ mu$ = 0 and $ sigma^2$ = $ sqrt { frac {2} {f_{in} + f_{out}}}$ | . He initialization / Kaiming initialization paper . RELU activations are mostly used - bercause they are robust to vanishing/ exploding gradients. | A more robust initialization technique was introduced by Kaiming et al. for activation functions like RELU. | both Xavier and He use similar theory → find a good variance for the distribution from which the initial parameters are drawn | This variance is adapted to the activation function used | derived without explicitly considering the type of the distribution . . | Red → He and Blue → Xavier | . | . Uniform: [$ pm sqrt { frac {6} {f_{in}} }$] . Normal: normal distribution * $ sqrt { frac {6} {f_{in}}}$ . or $ mu$ = 0 and $ sigma^2$ = $ sqrt{ frac{2}{f_{in}}} | . Remember . use Xavier for Sigmoid, tanh and Softmax | use He for ReLU and Leaky ReLU | . Resources . James Dellinger’s blog post | How to initialize deep neural networks? Xavier and Kaiming initialization | Daniel Godoy’s blog post | Keras initializers | Pytorch forums discussion | Krish Naik’s video | .",
            "url": "https://jithendray.github.io/blogy/machinelearning/deeplearning/2021/02/01/initialization.html",
            "relUrl": "/machinelearning/deeplearning/2021/02/01/initialization.html",
            "date": " • Feb 1, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Time series forecasting with python",
            "content": "Why this repository? . A lof of github repositories for time series forecasting use dummy series with strong and unrealistic features to showcase different models. This repository tries to give a more real use scenario on how to approach time series forecasting. . import matplotlib.pyplot as plt import matplotlib as mpl import pandas as pd #Basic library for all of our dataset operations import numpy as np import requests import io import statsmodels.tsa.api as smt import statsmodels as sm import tensorflow as tf import pmdarima as pm import warnings import xgboost as xgb import lightgbm as lgb import gluonts from math import sqrt import shap warnings.filterwarnings(&quot;ignore&quot;) #We will use deprecated models of statmodels which throw a lot of warnings to use more modern ones from utils.metrics import evaluate from utils.plots import bar_metrics from statsmodels.tsa.ar_model import AR from random import random from datetime import datetime from fbprophet import Prophet from statsmodels.tsa.seasonal import seasonal_decompose from statsmodels.graphics.tsaplots import plot_acf,plot_pacf from statsmodels.tsa.stattools import adfuller from statsmodels.tsa.holtwinters import SimpleExpSmoothing, ExponentialSmoothing from sklearn import linear_model, svm from sklearn.linear_model import LinearRegression from sklearn.ensemble import RandomForestRegressor from sklearn.neighbors import KNeighborsRegressor from sklearn.preprocessing import MinMaxScaler, StandardScaler from sklearn.metrics import mean_squared_error, mean_absolute_error, make_scorer from sklearn.model_selection import cross_val_score, GridSearchCV from math import sqrt from xgboost import plot_importance, plot_tree from gluonts.model.deepar import DeepAREstimator from gluonts.trainer import Trainer from gluonts.dataset.common import ListDataset from gluonts.evaluation.backtest import make_evaluation_predictions from itertools import islice from pylab import rcParams # progress bar from tqdm import tqdm, tqdm_notebook from bayes_opt import BayesianOptimization #Extra settings seed = 42 tf.random.set_seed(seed) np.random.seed(seed) plt.style.use(&#39;bmh&#39;) mpl.rcParams[&#39;axes.labelsize&#39;] = 14 mpl.rcParams[&#39;xtick.labelsize&#39;] = 12 mpl.rcParams[&#39;ytick.labelsize&#39;] = 12 mpl.rcParams[&#39;text.color&#39;] = &#39;k&#39; print(tf.__version__) . Bad key &#34;text.kerning_factor&#34; on line 4 in /home/jiwidi/anaconda3/envs/timeseries/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test_patch.mplstyle. You probably need to get an updated matplotlibrc file from https://github.com/matplotlib/matplotlib/blob/v3.1.3/matplotlibrc.template or from the matplotlib source distribution INFO:root:Using CPU . 2.0.0 . &#128218; Time series analysis and transforms . This notebook contains a set of operations we can perform in our time series in order to get some insights or transform the series to make forecasting easier. . Which ones will we touching in this notebook? . Time series decomposition . Level | Trend | Seasonality | Noise | . | Stationarity . AC and PAC plots | Rolling mean and std | Dickey-Fuller test | . | Making our time series stationary . Difference transform | Log scale | Smoothing | Moving average | . | . Load the dataset and quick preview . air_pollution = pd.read_csv(&#39;datasets/air_pollution.csv&#39;,parse_dates=[&#39;date&#39;]) air_pollution.set_index(&#39;date&#39;,inplace=True) air_pollution.head() # . pollution_today dew temp press wnd_spd snow rain pollution_yesterday . date . 2010-01-02 145.958333 | -8.500000 | -5.125000 | 1024.750000 | 24.860000 | 0.708333 | 0.0 | 10.041667 | . 2010-01-03 78.833333 | -10.125000 | -8.541667 | 1022.791667 | 70.937917 | 14.166667 | 0.0 | 145.958333 | . 2010-01-04 31.333333 | -20.875000 | -11.500000 | 1029.291667 | 111.160833 | 0.000000 | 0.0 | 78.833333 | . 2010-01-05 42.458333 | -24.583333 | -14.458333 | 1033.625000 | 56.920000 | 0.000000 | 0.0 | 31.333333 | . 2010-01-06 56.416667 | -23.708333 | -12.541667 | 1033.750000 | 18.511667 | 0.000000 | 0.0 | 42.458333 | . air_pollution.describe() . pollution_today dew temp press wnd_spd snow rain pollution_yesterday . count 1825.000000 | 1825.000000 | 1825.000000 | 1825.000000 | 1825.000000 | 1825.000000 | 1825.000000 | 1825.000000 | . mean 98.245080 | 1.828516 | 12.459041 | 1016.447306 | 23.894307 | 0.052763 | 0.195023 | 98.245080 | . std 76.807697 | 14.163508 | 11.552997 | 10.076053 | 41.373161 | 0.546072 | 0.993917 | 76.807697 | . min 3.166667 | -33.333333 | -14.458333 | 994.041667 | 1.412500 | 0.000000 | 0.000000 | 3.166667 | . 25% 42.333333 | -10.083333 | 1.541667 | 1007.916667 | 5.904167 | 0.000000 | 0.000000 | 42.333333 | . 50% 79.166667 | 2.041667 | 13.916667 | 1016.208333 | 10.953750 | 0.000000 | 0.000000 | 79.166667 | . 75% 131.166667 | 15.083333 | 23.166667 | 1024.541667 | 22.235000 | 0.000000 | 0.000000 | 131.166667 | . max 541.895833 | 26.208333 | 32.875000 | 1043.458333 | 463.187917 | 14.166667 | 17.583333 | 541.895833 | . Lets check each feature values . values = air_pollution.values groups = [0, 1, 2, 3, 4, 5, 6, 7] i = 1 # plot each column for group in groups: plt.subplot(len(groups), 1, i) plt.plot(values[:, group]) plt.title(air_pollution.columns[group], y=0.5, loc=&#39;right&#39;) i += 1 plt.show() . plt.figure(num=None, figsize=(30, 10), dpi=80, facecolor=&#39;w&#39;, edgecolor=&#39;k&#39;) plt.title(&#39;Air pollution&#39;,fontsize=30) plt.plot(air_pollution.pollution_today) plt.savefig(&quot;results/pollution.png&quot;) . Decomposing our time series . One of the most common analysis for time series is decomposing it into multiple parts. The parts we can divide a time series into are: level, trend, seasonality and noise, all series contain level and noise but seasonality and trend are not always present (there will be more analysis for this two parts). . This 4 parts can combine either additively or multiplicatively into the time series. . Additive Model . y(t) = Level + Trend + Seasonality + Noise . Additives models are lineal. Trend is linear and seasonality has constant frequency and amplitude. Change is constant over time . Multiplicative model . y(t) = Level * Trend * Seasonality * Noise . Multiplicatives models are nonlinear,trend is curved and seasonality is not constant. Change is not constant over time . Decomposing is used to analyse the time series. Identify each one of the different parts of the time series and its behaviour, each of the components may affect your models in different ways. . Most time series are a combination of a additive model and a multiplicate model, is hard to identify real world time series into one single model. . Automatic time series decomposition . Statsmodel python library provides a function seasonal_compose() to automatically decompose a time series, you still need to specify wether the model is additive or multiplicative. We will use multiplicative as our quick peak at the pm2.5 time series shows no linear trend. . rcParams[&#39;figure.figsize&#39;] = 18, 8 plt.figure(num=None, figsize=(50, 20), dpi=80, facecolor=&#39;w&#39;, edgecolor=&#39;k&#39;) series = air_pollution.pollution_today[:365] result = seasonal_decompose(series, model=&#39;multiplicative&#39;) result.plot() pass . &lt;Figure size 4000x1600 with 0 Axes&gt; . Level . Level simply means the current value of the series once we remove trend, seasonality and the random noise. This are the true values that come from the series itself and we will try to predict with our models. Most of the models will benefit the more our time series is composed by the level and not trends/seasonality/noise. We also present models capable of handling seasonality and trend (non stationary series) . Trend . A trend is observed when there is an increasing or decreasing slope observed in the time series. A trend is a smooth, general, long-term, average tendency. It is not always necessary that the increase or decrease is in the same direction throughout the given period of time. . Trend can be removed from your time series data (and data in the future) as a data preparation and cleaning exercise. This is common when using statistical methods for time series forecasting, but does not always improve results when using machine learning models. We will see different methods for this in the making your series stationary section . In practice, identifying a trend in a time series can be a subjective process as we are never sure if contains seasonalities or noise to it, Create line plots of your data and inspect the plots for obvious trends. . Now we will try some methods to check for trend in our series: . Automatic decomposing | Moving average | Fit a linear regression model to identify trend | . fig = plt.figure(figsize=(15, 7)) layout = (3,2) pm_ax = plt.subplot2grid(layout, (0,0), colspan=2) mv_ax = plt.subplot2grid(layout, (1,0), colspan=2) fit_ax = plt.subplot2grid(layout, (2,0), colspan=2) pm_ax.plot(result.trend) pm_ax.set_title(&quot;Automatic decomposed trend&quot;) mm = air_pollution.pollution_today.rolling(12).mean() mv_ax.plot(mm) mv_ax.set_title(&quot;Moving average 12 steps&quot;) X = [i for i in range(0, len(air_pollution.pollution_today))] X = np.reshape(X, (len(X), 1)) y = air_pollution.pollution_today.values model = LinearRegression() model.fit(X, y) # calculate trend trend = model.predict(X) fit_ax.plot(trend) fit_ax.set_title(&quot;Trend fitted by linear regression&quot;) plt.tight_layout() . We can see our series does not have a strong trend, results from both the automatic decomposition and the moving average look more like a seasonality efect+random noise than a trend. This sort of confirmed with our linear regression, which cant find our series properly and gives us a poor trend. . We could also try to split our series into smaller ones to try identify subtrends with the mentioned methods but we will not be doing so in this section. . Seasonality . Seasonality is observed when there is a distinct repeated pattern observed between regular intervals due to seasonal factors. It could be because of the month of the year, the day of the month, weekdays or even time of the day. For example the amount of sunscream protector (always low in winter and high in summer). . The automatic decomposing chart did not gave us a good look into the decomposed seasonality, let&#39;s try decomposing smaller parts of the time series first and test seasonalities we found into the others. . Lets go with the first year of data only now: . rcParams[&#39;figure.figsize&#39;] = 18, 8 plt.figure(num=None, figsize=(50, 20), dpi=80, facecolor=&#39;w&#39;, edgecolor=&#39;k&#39;) series = air_pollution.pollution_today[:365] result = seasonal_decompose(series, model=&#39;multiplicative&#39;) result.plot() pass . &lt;Figure size 4000x1600 with 0 Axes&gt; . Here can see a clear weekly trend, 4 spikes every month (weerkly). Lets check how the last year of data looks . rcParams[&#39;figure.figsize&#39;] = 18, 8 plt.figure(num=None, figsize=(50, 20), dpi=80, facecolor=&#39;w&#39;, edgecolor=&#39;k&#39;) series = air_pollution.pollution_today[-365:] result = seasonal_decompose(series, model=&#39;multiplicative&#39;) result.plot() pass . &lt;Figure size 4000x1600 with 0 Axes&gt; . We see another weekly seasonality(4 spikes between every month) but a bit different to the original one, this is something we should always expect from real datasets as their seasonalities will never be perfect but a combination of multiples. . INTERPRETATION . resample = air_pollution.resample(&#39;W&#39;) weekly_mean = resample.mean() weekly_mean.pollution_today.plot(label=&#39;Weekly mean&#39;) plt.title(&quot;Resampled series to weekly mean values&quot;) plt.legend() . &lt;matplotlib.legend.Legend at 0x7f82e0f57f98&gt; . Manual methods to find seasonalities . We can also try to generate a model to find the seasonalities for us. One of the most common to use is a simple polynomial model. . # fit polynomial: x^2*b1 + x*b2 + ... + bn series = air_pollution.pollution_today.values X = [i%365 for i in range(0, len(series))] y = series degree = 100 coef = np.polyfit(X, y, degree) # create curve curve = list() for i in range(len(X)): value = coef[-1] for d in range(degree): value += X[i]**(degree-d) * coef[d] curve.append(value) # plot curve over original data plt.plot(series,label=&#39;Original&#39;) plt.plot(curve, color=&#39;red&#39;, linewidth=3,label=&#39;polynomial model&#39;) plt.legend() plt.title(&quot;Polynomial fit to find seasonality&quot;) plt.show() . We can see how the model to find a seasonality fits poorly to our data. Is going to be a complicate time series to model :P . Noise . Our time series will also have a noise component to them, most likely white noise. We say white noise is present if the measurement are independent and identically distributed with a mean of zero. This will mean all our measurements have same variance and no correlation with the rest of values in the series. . If our time series has white noise this will mean we can&#39;t predict that component of the series (as is random) and we shoul aim to produce a model with errors close to this white noise. . How to check if our series has white noise? . Check our series histogram, does it look like a Gaussian distribution? Mean=0 and constand std | Correlation plots | Standard deviation distribution, is it a Gaussian distribution? | Does the mean or level change over time? | . fig = plt.figure(figsize=(12, 7)) layout = (2,2) hist_ax = plt.subplot2grid(layout, (0,0)) ac_ax = plt.subplot2grid(layout, (1,0)) hist_std_ax = plt.subplot2grid(layout, (0,1)) mean_ax = plt.subplot2grid(layout, (1,1)) air_pollution.pollution_today.hist(ax=hist_ax) hist_ax.set_title(&quot;Original series histogram&quot;) plot_acf(series, lags = 30,ax = ac_ax) ac_ax.set_title(&quot;Autocorrelation&quot;) mm = air_pollution.pollution_today.rolling(7).std() mm.hist(ax=hist_std_ax) hist_std_ax.set_title(&quot;Standard deviation histogram&quot;) mm = air_pollution.pollution_today.rolling(30).mean() mm.plot(ax=mean_ax) mean_ax.set_title(&quot;Mean over time&quot;) . Text(0.5, 1.0, &#39;Mean over time&#39;) . We can see our series do not follow a Gaussian distribution from the histogram and neither the standard deviation, thought the std does has the mean more centered which shows a small part of white noise that is not possible to split from the original series (this will happen most of the times, specially is real life datasets)). . We also have a small correlation with close measurements in time but not present with distant measurements (this could also indicate low seasonality). The mean over time also shows something similar with a constant value and high peaks in the same moments for the 4 years (smaller in 2012) . We could say our series does contain a small part of white noise but it is really small and hard to remove . Stationarity . Stationarity is an important characteristic of time series. A time series is stationarity if it has constant mean and variance over time. Most models work only with stationary data as this makes it easier to model. Not all time series are stationary but we can transform them into stationary series in different ways. . Often, stock prices are not a stationary process, since we might see a growing trend, or its volatility might increase over time (meaning that variance is changing). . Check for sationarity . Autocorrelation and Partial autocorrelation plots . Autocorelation plots show how correlated are values at time t with the next values in time t+1,t+2,..t+n. If the data would be non-stationary the autocorrelation values will be highly correlated with distant points in time showing possible seasonalities or trends. . Stationary series autocorrelation values will quickly decrease over time t. This shows us that no information is carried over time and then the series should be constant over time. . plot_acf(series, lags = 30) plot_pacf(series, lags = 30) plt.show() . We saw that our time series values are not correlated with distant points in time, this is good and shows us our series should be stationary but for the shake of learning and confirming we will test with some other methods . Rolling means and standard deviation of our series . We were talking about how our mean and standard deviation should be constant over time in order to have a stationary time series, why not just plot this two properties? . rolmean = air_pollution.pollution_today.rolling(window=12).mean() rolstd = air_pollution.pollution_today.rolling(window=12).std() #Plot rolling statistics: orig = plt.plot(air_pollution.pollution_today,label=&#39;Original&#39;) mean = plt.plot(rolmean, color=&#39;red&#39;, label=&#39;Rolling Mean&#39;) std = plt.plot(rolstd, color=&#39;black&#39;, label = &#39;Rolling Std&#39;) plt.legend(loc=&#39;best&#39;) plt.title(&#39;Rolling Mean &amp; Standard Deviation&#39;) plt.show(block=False) . We can see how our mean and standar deviation have a constant behaviour over the years, even if they change over the year this behaviour is then repeated next year. This proves us again a stationary series . Augmented Dickey-Fuller test . The Augmented Dickey-Fuller test is a type of statistical test called a unit root test. The intuition behind a unit root test is that it determines how strongly a time series is defined by a trend. There are a number of unit root tests and the Augmented Dickey-Fuller may be one of the more widely used. It uses an autoregressive model and optimizes an information criterion across multiple different lag values. . The null hypothesis of the test is that the time series can be represented by a unit root, that it is not stationary (has some time-dependent structure). The alternate hypothesis (rejecting the null hypothesis) is that the time series is stationary. . Null Hypothesis (H0): If failed to be rejected, it suggests the time series has a unit root, meaning it is non-stationary. It has some time dependent structure. Alternate Hypothesis (H1): The null hypothesis is rejected; it suggests the time series does not have a unit root, meaning it is stationary. It does not have time-dependent structure. We interpret this result using the p-value from the test. A p-value below a threshold (such as 5% or 1%) suggests we reject the null hypothesis (stationary), otherwise a p-value above the threshold suggests we fail to reject the null hypothesis (non-stationary). . p-value &gt; 0.05: Fail to reject the null hypothesis (H0), the data has a unit root and is non-stationary. p-value &lt;= 0.05: Reject the null hypothesis (H0), the data does not have a unit root and is stationary. Below is an example of calculating the Augmented Dickey-Fuller test on the Daily Female Births dataset. The statsmodels library provides the adfuller() function that implements the test. . X = air_pollution.pollution_today.values result = adfuller(X) print(&#39;ADF Statistic: %f&#39; % result[0]) print(&#39;p-value: %f&#39; % result[1]) print(&#39;Critical Values:&#39;) for key, value in result[4].items(): print(&#39; t%s: %.3f&#39; % (key, value)) . ADF Statistic: -10.116719 p-value: 0.000000 Critical Values: 1%: -3.434 5%: -2.863 10%: -2.568 . Here we also provide a method to quickly perform all the previous methods into one single function call and a pretty graph :) . def tsplot(y, lags=None, figsize=(12, 7), syle=&#39;bmh&#39;): if not isinstance(y, pd.Series): y = pd.Series(y) with plt.style.context(style=&#39;bmh&#39;): fig = plt.figure(figsize=(12, 7)) layout = (3,2) ts_ax = plt.subplot2grid(layout, (0,0), colspan=2) acf_ax = plt.subplot2grid(layout, (1,0)) pacf_ax = plt.subplot2grid(layout, (1,1)) mean_std_ax = plt.subplot2grid(layout, (2,0), colspan=2) y.plot(ax=ts_ax) p_value = sm.tsa.stattools.adfuller(y)[1] hypothesis_result = &quot;We reject stationarity&quot; if p_value&lt;=0.05 else &quot;We can not reject stationarity&quot; ts_ax.set_title(&#39;Time Series stationary analysis Plots n Dickey-Fuller: p={0:.5f} Result: {1}&#39;.format(p_value,hypothesis_result)) smt.graphics.plot_acf(y, lags=lags, ax=acf_ax) smt.graphics.plot_pacf(y, lags=lags, ax=pacf_ax) plt.tight_layout() rolmean = air_pollution.pollution_today.rolling(window=12).mean() rolstd = air_pollution.pollution_today.rolling(window=12).std() #Plot rolling statistics: orig = plt.plot(air_pollution.pollution_today,label=&#39;Original&#39;) mean = plt.plot(rolmean, color=&#39;red&#39;, label=&#39;Rolling Mean&#39;) std = plt.plot(rolstd, color=&#39;black&#39;, label = &#39;Rolling Std&#39;) plt.legend(loc=&#39;best&#39;) plt.title(&#39;Rolling Mean &amp; Standard Deviation&#39;) tsplot(air_pollution.pollution_today, lags=30) . Making Time Series Stationary . Okay we got lucky with this dataset and is already stationary, but what happens when this is not the case? We included a dummy dataset called international_airline_passengers.csv on the datasets folders which is not stationary and we will apply some methods in this section to transform it into a stationary series. . passengers = pd.read_csv(&quot;datasets/international_airline_passengers.csv&quot;) passengers.passengers.plot(label=&#39;Original&#39;) passengers.passengers.rolling(window=12).mean().plot(color=&#39;red&#39;, label=&#39;Windowed mean&#39;) passengers.passengers.rolling(window=12).std().plot(color=&#39;black&#39;, label=&#39;Std mean&#39;) plt.legend() plt.title(&#39;Original vs Windowed mean vs Windowed std&#39;) . Text(0.5, 1.0, &#39;Original vs Windowed mean vs Windowed std&#39;) . Lets run our stationary multitest function over this series . tsplot(passengers.passengers, lags=30) . With a p value of ~1 and high correlation values over time distant samples (showing a clear seasonality shape) we need to apply some methods to make the series stationary. . Coming back to the stationary definition, what makes our current series non stationary? . Trend - The mean for our series is not constant, it increases over time and . Seasonality - The values of our series vary over time with an specific pattern that repeats over time, this is called seasonalities (spike of people flying on the 24th of December) . We now present some methods to remove or smotth this trend and seasonality components . Difference transform . Applying a difference transform to a time series could help remove the series dependence on time. . This transform is done by substracting the previous obesvation to the current one. . difference(t) = observation(t) - observation(t-1) . Taking the difference between consecutive observations would be a lag-1 difference, we can tweek this lag value to fit our series. . We can also apply differencing transforms consecutively in the same series if the temporal effect hasnt been removed yet. This is called multiple order difference transform . def difference(dataset, interval=1, order=1): for u in range(order): diff = list() for i in range(interval, len(dataset)): value = dataset[i] - dataset[i - interval] diff.append(value) dataset=diff return diff . lag1series = pd.Series(difference(passengers.passengers, interval=1, order=1)) lag3series = pd.Series(difference(passengers.passengers, interval=3, order=1)) lag1order2series = pd.Series(difference(passengers.passengers, interval=1, order=2)) fig = plt.figure(figsize=(14,11)) layout = (3,2) original = plt.subplot2grid(layout, (0,0), colspan=2) lag1 = plt.subplot2grid(layout, (1,0)) lag3 = plt.subplot2grid(layout, (1,1)) lag1order2 = plt.subplot2grid(layout, (2,0), colspan=2) original.set_title(&#39;Original series&#39;) original.plot(passengers.passengers, label = &#39;Original&#39;) original.plot(passengers.passengers.rolling(7).mean(), color=&#39;red&#39;, label=&#39;Rolling Mean&#39;) original.plot(passengers.passengers.rolling(7).std(), color=&#39;black&#39;, label = &#39;Rolling Std&#39;) original.legend(loc=&#39;best&#39;) lag1.set_title(&#39;Difference series with lag 1 order 1&#39;) lag1.plot(lag1series, label = &quot;Lag1&quot;) lag1.plot(lag1series.rolling(7).mean(), color=&#39;red&#39;, label=&#39;Rolling Mean&#39;) lag1.plot(lag1series.rolling(7).std(), color=&#39;black&#39;, label = &#39;Rolling Std&#39;) lag1.legend(loc=&#39;best&#39;) lag3.set_title(&#39;Difference series with lag 3 order 1&#39;) lag3.plot(lag3series, label = &quot;Lag3&quot;) lag3.plot(lag3series.rolling(7).mean(), color=&#39;red&#39;, label=&#39;Rolling Mean&#39;) lag3.plot(lag3series.rolling(7).std(), color=&#39;black&#39;, label = &#39;Rolling Std&#39;) lag3.legend(loc=&#39;best&#39;) lag1order2.set_title(&#39;Difference series with lag 1 order 2&#39;) lag1order2.plot(lag1order2series, label = &quot;Lag1order2&quot;) lag1order2.plot(lag1order2series.rolling(7).mean(), color=&#39;red&#39;, label=&#39;Rolling Mean&#39;) lag1order2.plot(lag1order2series.rolling(7).std(), color=&#39;black&#39;, label = &#39;Rolling Std&#39;) lag1order2.legend(loc=&#39;best&#39;) . &lt;matplotlib.legend.Legend at 0x7f82e098bf28&gt; . We can see how 1 order differencing doesnt really remove stationary but once we go with a order 2 difference it looks closer to a stationary series . Log scale transformation . Applying a log scale transform to a time series could also help remove the series dependence on time. . This transform is done by substracting the previous obesvation to the current one. . LogScaleTransform(t)= Log(t) . ts_log = np.log(passengers.passengers) ts_log.plot(label=&#39;Log scale result&#39;) ts_log.rolling(window=12).mean().plot(color=&#39;red&#39;, label=&#39;Windowed mean&#39;) ts_log.rolling(window=12).std().plot(color=&#39;black&#39;, label=&#39;Std mean&#39;) plt.legend() plt.title(&#39;Log scale transformation into original series&#39;) . Text(0.5, 1.0, &#39;Log scale transformation into original series&#39;) . . Smoothing . We have seen the moving mean as a measure to check stationarity, we can also apply this windows to our series to remove seasonality. . With smotthing we will take rolling averages over periods of time. Is a bit tricky to choose the best windows #MORE ON THIS IN NEXT SECTION WITH AUTO WINDOWS . avg = pd.Series(ts_log).rolling(12).mean() plt.plot(avg, label=&#39;Log scale smoothed with windows 12&#39;) avg.rolling(window=12).mean().plot(color=&#39;red&#39;, label=&#39;Windowed mean&#39;) avg.rolling(window=12).std().plot(color=&#39;black&#39;, label=&#39;Std mean&#39;) plt.legend() . &lt;matplotlib.legend.Legend at 0x7f82e0ad1668&gt; . We can combine it with our previous log scale and apply differencing . ts_log_moving_avg_diff = ts_log - avg ts_log_moving_avg_diff.plot(label=&#39;Original&#39;) ts_log_moving_avg_diff.rolling(12).mean().plot(color=&#39;red&#39;,label=&quot;Rolling mean&quot;) ts_log_moving_avg_diff.rolling(12).std().plot(color=&#39;black&#39;,label=&quot;Rolling mean&quot;) plt.legend(loc=&#39;best&#39;) . &lt;matplotlib.legend.Legend at 0x7f82e0a8b4e0&gt; . Methods for time series forecasting . There are many methods that we can use for time series forecasting and there is not a clear winner. Model selection should always depend on how you data look and what are you trying to achieve. Some models may be more robust against outliers but perform worse than the more sensible and could still be the best choice depending on the use case. . When looking at your data the main split is wether we have extra regressors (features) to our time series or just the series. Based on this we can start exploring different methods for forecasting and their performance in different metrics. . In this section we will show models for both cases, time series with and without extra regressors. . Prepare data before modeling . resultsDict={} predictionsDict={} split_date =&#39;2014-01-01&#39; df_training = air_pollution.loc[air_pollution.index &lt;= split_date] df_test = air_pollution.loc[air_pollution.index &gt; split_date] print(f&quot;{len(df_training)} days of training data n {len(df_test)} days of testing data &quot;) . 1461 days of training data 364 days of testing data . It is also very important to include some naive forecast as the series mean or previous value to make sure our models perform better than the simplest of the simplest. We dont want to introduce any complexity if it does not provides any performance gain. . mean = df_test.pollution_today.mean() mean = np.array([mean for u in range(len(df_test))]) resultsDict[&#39;Naive mean&#39;] = evaluate(df_test.pollution_today, mean) predictionsDict[&#39;Naive mean&#39;] = mean resultsDict[&#39;Yesterdays value&#39;] = evaluate(df_test.pollution_today, df_test.pollution_yesterday) predictionsDict[&#39;Yesterdays value&#39;] = df_test.pollution_yesterday.values . Univariate-time-series-forecasting . In this section we will focus on time series forecasting methods capable of only looking at the target variable. This means no other regressors (more variables) can be added into the model. . Simple Exponential Smoothing (SES) . The Simple Exponential Smoothing (SES) method models the next time step as an exponentially weighted linear function of observations at prior time steps. This method expects our time series to be non stationary in order to perform adecuately (no trend or seasonality) . index = len(df_training) yhat = list() for t in tqdm(range(len(df_test.pollution_today))): temp_train = air_pollution[:len(df_training)+t] model = SimpleExpSmoothing(temp_train.pollution_today) model_fit = model.fit() predictions = model_fit.predict(start=len(temp_train), end=len(temp_train)) yhat = yhat + [predictions] yhat = pd.concat(yhat) resultsDict[&#39;SES&#39;] = evaluate(df_test.pollution_today, yhat.values) predictionsDict[&#39;SES&#39;] = yhat.values . 100%|██████████| 364/364 [00:02&lt;00:00, 123.10it/s] . Holt Winter&#8217;s Exponential Smoothing (HWES) . HWES or also known as triple exponential smoothing . index = len(df_training) yhat = list() for t in tqdm(range(len(df_test.pollution_today))): temp_train = air_pollution[:len(df_training)+t] model = ExponentialSmoothing(temp_train.pollution_today) model_fit = model.fit() predictions = model_fit.predict(start=len(temp_train), end=len(temp_train)) yhat = yhat + [predictions] yhat = pd.concat(yhat) resultsDict[&#39;HWES&#39;] = evaluate(df_test.pollution_today, yhat.values) predictionsDict[&#39;HWES&#39;] = yhat.values . 100%|██████████| 364/364 [00:03&lt;00:00, 121.15it/s] . Autoregression (AR) . The autoregression (AR) method models the next step in the sequence as a linear function of the observations at prior time steps. Parameters of the model: . Number of AR (Auto-Regressive) terms (p): p is the parameter associated with the auto-regressive aspect of the model, which incorporates past values i.e lags of dependent variable. For instance if p is 5, the predictors for x(t) will be x(t-1)….x(t-5). | . index = len(df_training) yhat = list() for t in tqdm(range(len(df_test.pollution_today))): temp_train = air_pollution[:len(df_training)+t] model = AR(temp_train.pollution_today) model_fit = model.fit() predictions = model_fit.predict(start=len(temp_train), end=len(temp_train), dynamic=False) yhat = yhat + [predictions] yhat = pd.concat(yhat) resultsDict[&#39;AR&#39;] = evaluate(df_test.pollution_today, yhat.values) predictionsDict[&#39;AR&#39;] = yhat.values . 100%|██████████| 364/364 [00:05&lt;00:00, 67.40it/s] . plt.plot(df_test.pollution_today.values, label=&#39;Original&#39;) plt.plot(yhat.values,color=&#39;red&#39;,label=&#39;AR predicted&#39;) plt.legend() . &lt;matplotlib.legend.Legend at 0x7f82e2deef60&gt; . Moving Average (MA) . The Moving Average (MA) method models the next step in the sequence as the average of a window of observations at prior time steps. Parameters of the model: . Number of MA (Moving Average) terms (q): q is size of the moving average part window of the model i.e. lagged forecast errors in prediction equation. For instance if q is 5, the predictors for x(t) will be e(t-1)….e(t-5) where e(i) is the difference between the moving average at ith instant and actual value. | . from statsmodels.tsa.arima_model import ARMA from random import random # Walk throught the test data, training and predicting 1 day ahead for all the test data index = len(df_training) yhat = list() for t in tqdm(range(len(df_test.pollution_today))): temp_train = air_pollution[:len(df_training)+t] model = ARMA(temp_train.pollution_today, order=(0, 1)) model_fit = model.fit(disp=False) predictions = model_fit.predict(start=len(temp_train), end=len(temp_train), dynamic=False) yhat = yhat + [predictions] yhat = pd.concat(yhat) resultsDict[&#39;MA&#39;] = evaluate(df_test.pollution_today, yhat.values) predictionsDict[&#39;MA&#39;] = yhat.values . 100%|██████████| 364/364 [00:12&lt;00:00, 29.02it/s] . plt.plot(df_test.pollution_today.values , label=&#39;Original&#39;) plt.plot(yhat.values,color=&#39;red&#39;,label=&#39;MA predicted&#39;) plt.legend() . &lt;matplotlib.legend.Legend at 0x7f82e0dc7400&gt; . Autoregressive Moving Average (ARMA) . This method will basically join the previous two AR and MA. Model parameters will be the sum of the two. . Number of AR (Auto-Regressive) terms (p): p is the parameter associated with the auto-regressive aspect of the model, which incorporates past values i.e lags of dependent variable. For instance if p is 5, the predictors for x(t) will be x(t-1)….x(t-5). | Number of MA (Moving Average) terms (q): q is size of the moving average part window of the model i.e. lagged forecast errors in prediction equation. For instance if q is 5, the predictors for x(t) will be e(t-1)….e(t-5) where e(i) is the difference between the moving average at ith instant and actual value. | . from statsmodels.tsa.arima_model import ARMA from random import random # Walk throught the test data, training and predicting 1 day ahead for all the test data index = len(df_training) yhat = list() for t in tqdm(range(len(df_test.pollution_today))): temp_train = air_pollution[:len(df_training)+t] model = ARMA(temp_train.pollution_today, order=(1, 1)) model_fit = model.fit(disp=False) predictions = model_fit.predict(start=len(temp_train), end=len(temp_train), dynamic=False) yhat = yhat + [predictions] yhat = pd.concat(yhat) resultsDict[&#39;ARMA&#39;] = evaluate(df_test.pollution_today, yhat.values) predictionsDict[&#39;ARMA&#39;] = yhat.values . 100%|██████████| 364/364 [01:04&lt;00:00, 5.62it/s] . plt.plot(df_test.pollution_today.values , label=&#39;Original&#39;) plt.plot(yhat.values,color=&#39;red&#39;,label=&#39;ARMA predicted&#39;) plt.legend() . &lt;matplotlib.legend.Legend at 0x7f82e121e6d8&gt; . Autoregressive integrated moving average (ARIMA) . In an ARIMA model there are 3 parameters that are used to help model the major aspects of a times series: seasonality, trend, and noise. These parameters are labeled p,d,and q. . Number of AR (Auto-Regressive) terms (p): p is the parameter associated with the auto-regressive aspect of the model, which incorporates past values i.e lags of dependent variable. For instance if p is 5, the predictors for x(t) will be x(t-1)….x(t-5). | Number of Differences (d): d is the parameter associated with the integrated part of the model, which effects the amount of differencing to apply to a time series. | Number of MA (Moving Average) terms (q): q is size of the moving average part window of the model i.e. lagged forecast errors in prediction equation. For instance if q is 5, the predictors for x(t) will be e(t-1)….e(t-5) where e(i) is the difference between the moving average at ith instant and actual value. | . Tuning ARIMA parameters . Non stationarity series will require level of differencing (d) &gt;0 in ARIMA Select the lag values for the Autoregression (AR) and Moving Average (MA) parameters, p and q respectively, using PACF, ACF plots AUTOARIMA . Note: A problem with ARIMA is that it does not support seasonal data. That is a time series with a repeating cycle. ARIMA expects data that is either not seasonal or has the seasonal component removed, e.g. seasonally adjusted via methods such as seasonal differencing. . from statsmodels.tsa.arima_model import ARIMA from sklearn.metrics import mean_squared_error from math import sqrt # Walk throught the test data, training and predicting 1 day ahead for all the test data index = len(df_training) yhat = list() for t in tqdm(range(len(df_test.pollution_today))): temp_train = air_pollution[:len(df_training)+t] model = ARIMA(temp_train.pollution_today, order=(1,0, 0)) model_fit = model.fit(disp=False) predictions = model_fit.predict(start=len(temp_train), end=len(temp_train), dynamic=False) yhat = yhat + [predictions] yhat = pd.concat(yhat) resultsDict[&#39;ARIMA&#39;] = evaluate(df_test.pollution_today, yhat.values) predictionsDict[&#39;ARIMA&#39;] = yhat.values . 100%|██████████| 364/364 [00:13&lt;00:00, 27.73it/s] . plt.plot(df_test.pollution_today.values , label=&#39;Original&#39;) plt.plot(yhat.values,color=&#39;red&#39;,label=&#39;ARIMA predicted&#39;) plt.legend() . &lt;matplotlib.legend.Legend at 0x7f82e0dd49b0&gt; . Auto ARIMA . autoModel = pm.auto_arima(df_training.pollution_today, trace=True, error_action=&#39;ignore&#39;, suppress_warnings=True,seasonal=False) autoModel.fit(df_training.pollution_today) . Fit ARIMA: order=(2, 0, 2); AIC=16198.338, BIC=16230.059, Fit time=0.305 seconds Fit ARIMA: order=(0, 0, 0); AIC=16788.406, BIC=16798.980, Fit time=0.002 seconds Fit ARIMA: order=(1, 0, 0); AIC=16227.306, BIC=16243.167, Fit time=0.029 seconds Fit ARIMA: order=(0, 0, 1); AIC=16263.917, BIC=16279.778, Fit time=0.026 seconds Fit ARIMA: order=(1, 0, 2); AIC=16196.487, BIC=16222.921, Fit time=0.222 seconds Fit ARIMA: order=(1, 0, 1); AIC=16194.487, BIC=16215.635, Fit time=0.142 seconds Fit ARIMA: order=(2, 0, 1); AIC=16196.487, BIC=16222.921, Fit time=0.254 seconds Total fit time: 0.982 seconds . ARIMA(callback=None, disp=0, maxiter=None, method=None, order=(1, 0, 1), out_of_sample_size=0, scoring=&#39;mse&#39;, scoring_args={}, seasonal_order=None, solver=&#39;lbfgs&#39;, start_params=None, suppress_warnings=True, transparams=True, trend=None, with_intercept=True) . order = autoModel.order yhat = list() for t in tqdm(range(len(df_test.pollution_today))): temp_train = air_pollution[:len(df_training)+t] model = ARIMA(temp_train.pollution_today, order=order) model_fit = model.fit(disp=False) predictions = model_fit.predict(start=len(temp_train), end=len(temp_train), dynamic=False) yhat = yhat + [predictions] yhat = pd.concat(yhat) resultsDict[&#39;AutoARIMA {0}&#39;.format(order)] = evaluate(df_test.pollution_today, yhat) predictionsDict[&#39;AutoARIMA {0}&#39;.format(order)] = yhat.values . 100%|██████████| 364/364 [01:07&lt;00:00, 5.43it/s] . plt.plot(df_test.pollution_today.values , label=&#39;Original&#39;) plt.plot(yhat.values,color=&#39;red&#39;,label=&#39;AutoARIMA {0}&#39;.format(order)) plt.legend() . &lt;matplotlib.legend.Legend at 0x7f82e0eca710&gt; . Seasonal Autoregressive Integrated Moving-Average (SARIMA) . Seasonal Autoregressive Integrated Moving Average, SARIMA or Seasonal ARIMA, is an extension of ARIMA that explicitly supports univariate time series data with a seasonal component. . It adds three new hyperparameters to specify the autoregression (AR), differencing (I) and moving average (MA) for the seasonal component of the series, as well as an additional parameter for the period of the seasonality. . Trend Elements: . There are three trend elements that require configuration. They are the same as the ARIMA model, specifically: . p: Trend autoregression order. | d: Trend difference order. | q: Trend moving average order. | . Seasonal Elements: . There are four seasonal elements that are not part of ARIMA that must be configured; they are: . P: Seasonal autoregressive order. | D: Seasonal difference order. | Q: Seasonal moving average order. | m: The number of time steps for a single seasonal period. For example, an S of 12 for monthly data suggests a yearly seasonal cycle. | . SARIMA notation: SARIMA(p,d,q)(P,D,Q,m) . from statsmodels.tsa.statespace.sarimax import SARIMAX # Walk throught the test data, training and predicting 1 day ahead for all the test data index = len(df_training) yhat = list() for t in tqdm(range(len(df_test.pollution_today))): temp_train = air_pollution[:len(df_training)+t] model = SARIMAX(temp_train.pollution_today, order=(1, 0, 0), seasonal_order=(0, 0, 0, 3)) model_fit = model.fit(disp=False) predictions = model_fit.predict(start=len(temp_train), end=len(temp_train), dynamic=False) yhat = yhat + [predictions] yhat = pd.concat(yhat) resultsDict[&#39;SARIMAX&#39;] = evaluate(df_test.pollution_today, yhat.values) predictionsDict[&#39;SARIMAX&#39;] = yhat.values . 100%|██████████| 364/364 [00:12&lt;00:00, 28.75it/s] . plt.plot(df_test.pollution_today.values , label=&#39;Original&#39;) plt.plot(yhat.values,color=&#39;red&#39;,label=&#39;SARIMAX&#39;) plt.legend() . &lt;matplotlib.legend.Legend at 0x7f82e0c3cb38&gt; . Auto - SARIMA . auto_arima documentation for selecting best model . autoModel = pm.auto_arima(df_training.pollution_today, trace=True, error_action=&#39;ignore&#39;, suppress_warnings=True, seasonal=True, m=6, stepwise=True) autoModel.fit(df_training.pollution_today) . Fit ARIMA: order=(2, 0, 2) seasonal_order=(1, 0, 1, 6); AIC=16199.943, BIC=16242.238, Fit time=6.298 seconds Fit ARIMA: order=(0, 0, 0) seasonal_order=(0, 0, 0, 6); AIC=16788.406, BIC=16798.980, Fit time=0.054 seconds Fit ARIMA: order=(1, 0, 0) seasonal_order=(1, 0, 0, 6); AIC=16229.161, BIC=16250.309, Fit time=0.513 seconds Fit ARIMA: order=(0, 0, 1) seasonal_order=(0, 0, 1, 6); AIC=16265.917, BIC=16287.064, Fit time=1.238 seconds Fit ARIMA: order=(2, 0, 2) seasonal_order=(0, 0, 1, 6); AIC=16200.349, BIC=16237.357, Fit time=1.454 seconds Fit ARIMA: order=(2, 0, 2) seasonal_order=(2, 0, 1, 6); AIC=16203.879, BIC=16251.461, Fit time=4.275 seconds Fit ARIMA: order=(2, 0, 2) seasonal_order=(1, 0, 0, 6); AIC=16200.355, BIC=16237.364, Fit time=1.408 seconds Fit ARIMA: order=(2, 0, 2) seasonal_order=(1, 0, 2, 6); AIC=16203.861, BIC=16251.443, Fit time=5.028 seconds Fit ARIMA: order=(2, 0, 2) seasonal_order=(0, 0, 0, 6); AIC=16198.453, BIC=16230.175, Fit time=0.250 seconds Fit ARIMA: order=(1, 0, 2) seasonal_order=(0, 0, 0, 6); AIC=16196.487, BIC=16222.921, Fit time=0.774 seconds Fit ARIMA: order=(1, 0, 1) seasonal_order=(0, 0, 0, 6); AIC=16194.487, BIC=16215.635, Fit time=0.377 seconds Fit ARIMA: order=(1, 0, 1) seasonal_order=(1, 0, 0, 6); AIC=16196.379, BIC=16222.814, Fit time=2.295 seconds Fit ARIMA: order=(1, 0, 1) seasonal_order=(0, 0, 1, 6); AIC=16196.372, BIC=16222.807, Fit time=2.154 seconds Fit ARIMA: order=(1, 0, 1) seasonal_order=(1, 0, 1, 6); AIC=16198.261, BIC=16229.982, Fit time=1.661 seconds Fit ARIMA: order=(0, 0, 1) seasonal_order=(0, 0, 0, 6); AIC=16263.917, BIC=16279.778, Fit time=0.285 seconds Fit ARIMA: order=(2, 0, 1) seasonal_order=(0, 0, 0, 6); AIC=16196.487, BIC=16222.921, Fit time=0.637 seconds Fit ARIMA: order=(1, 0, 0) seasonal_order=(0, 0, 0, 6); AIC=16227.307, BIC=16243.168, Fit time=0.051 seconds Total fit time: 28.769 seconds . ARIMA(callback=None, disp=0, maxiter=None, method=None, order=(1, 0, 1), out_of_sample_size=0, scoring=&#39;mse&#39;, scoring_args={}, seasonal_order=(0, 0, 0, 6), solver=&#39;lbfgs&#39;, start_params=None, suppress_warnings=True, transparams=True, trend=None, with_intercept=True) . order = autoModel.order seasonalOrder = autoModel.seasonal_order yhat = list() for t in tqdm(range(len(df_test.pollution_today))): temp_train = air_pollution[:len(df_training)+t] model = SARIMAX(temp_train.pollution_today, order=order, seasonal_order=seasonalOrder) model_fit = model.fit(disp=False) predictions = model_fit.predict(start=len(temp_train), end=len(temp_train), dynamic=False) yhat = yhat + [predictions] yhat = pd.concat(yhat) resultsDict[&#39;AutoSARIMAX {0},{1}&#39;.format(order,seasonalOrder)] = evaluate(df_test.pollution_today, yhat.values) predictionsDict[&#39;AutoSARIMAX {0},{1}&#39;.format(order,seasonalOrder)] = yhat.values . 100%|██████████| 364/364 [00:25&lt;00:00, 14.41it/s] . plt.plot(df_test.pollution_today.values , label=&#39;Original&#39;) plt.plot(yhat.values,color=&#39;red&#39;,label=&#39;SARIMAX&#39;) plt.legend() . &lt;matplotlib.legend.Legend at 0x7f82e0ea2da0&gt; . Prophet . Prophet is a model released by facebook. Is essentially a curve fitting approach, very similar in spirit to how BSTS models trend and seasonality, except that it uses generalized additive models instead of a state-space representation to describe each component. . prophet_training = df_training.rename(columns={&#39;pollution_today&#39;: &#39;y&#39;}) # old method prophet_training[&#39;ds&#39;] = prophet_training.index prophet_training.index = pd.RangeIndex(len(prophet_training.index)) prophet_test = df_test.rename(columns={&#39;pollution_today&#39;: &#39;y&#39;}) # old method prophet_test[&#39;ds&#39;] = prophet_test.index prophet_test.index = pd.RangeIndex(len(prophet_test.index)) . prophet = Prophet( growth=&#39;linear&#39;, seasonality_mode=&#39;multiplicative&#39;, holidays_prior_scale=20, daily_seasonality=False, weekly_seasonality=False, yearly_seasonality=False ).add_seasonality( name=&#39;monthly&#39;, period=30.5, fourier_order=55 ).add_seasonality( name=&#39;daily&#39;, period=1, fourier_order=15 ).add_seasonality( name=&#39;weekly&#39;, period=7, fourier_order=25 ).add_seasonality( name=&#39;yearly&#39;, period=365.25, fourier_order=20 ).add_seasonality( name=&#39;quarterly&#39;, period=365.25/4, fourier_order=55 ).add_country_holidays(country_name=&#39;China&#39;) . prophet.fit(prophet_training) yhat = prophet.predict(prophet_test) resultsDict[&#39;Prophet univariate&#39;] = evaluate(df_test.pollution_today, yhat.yhat.values) predictionsDict[&#39;Prophet univariate&#39;] = yhat.yhat.values . plt.plot(df_test.pollution_today.values , label=&#39;Original&#39;) plt.plot(yhat.yhat,color=&#39;red&#39;,label=&#39;Prophet univariate&#39;) plt.legend() . &lt;matplotlib.legend.Legend at 0x7f82e07eef28&gt; . Multivariate time series forecasting . def create_time_features(df,target=None): &quot;&quot;&quot; Creates time series features from datetime index &quot;&quot;&quot; df[&#39;date&#39;] = df.index df[&#39;hour&#39;] = df[&#39;date&#39;].dt.hour df[&#39;dayofweek&#39;] = df[&#39;date&#39;].dt.dayofweek df[&#39;quarter&#39;] = df[&#39;date&#39;].dt.quarter df[&#39;month&#39;] = df[&#39;date&#39;].dt.month df[&#39;year&#39;] = df[&#39;date&#39;].dt.year df[&#39;dayofyear&#39;] = df[&#39;date&#39;].dt.dayofyear df[&#39;sin_day&#39;] = np.sin(df[&#39;dayofyear&#39;]) df[&#39;cos_day&#39;] = np.cos(df[&#39;dayofyear&#39;]) df[&#39;dayofmonth&#39;] = df[&#39;date&#39;].dt.day df[&#39;weekofyear&#39;] = df[&#39;date&#39;].dt.weekofyear X = df.drop([&#39;date&#39;],axis=1) if target: y = df[target] X = X.drop([target],axis=1) return X, y return X . X_train_df, y_train = create_time_features(df_training, target=&#39;pollution_today&#39;) X_test_df, y_test = create_time_features(df_test, target=&#39;pollution_today&#39;) scaler = StandardScaler() scaler.fit(X_train_df) #No cheating, never scale on the training+test! X_train = scaler.transform(X_train_df) X_test = scaler.transform(X_test_df) X_train_df = pd.DataFrame(X_train,columns=X_train_df.columns) X_test_df = pd.DataFrame(X_test,columns=X_test_df.columns) . Linear models . Bayesian regression . reg = linear_model.BayesianRidge() reg.fit(X_train, y_train) yhat = reg.predict(X_test) resultsDict[&#39;BayesianRidge&#39;] = evaluate(df_test.pollution_today, yhat) predictionsDict[&#39;BayesianRidge&#39;] = yhat . Lasso . reg = linear_model.Lasso(alpha=0.1) reg.fit(X_train, y_train) yhat = reg.predict(X_test) resultsDict[&#39;Lasso&#39;] = evaluate(df_test.pollution_today, yhat) predictionsDict[&#39;Lasso&#39;] = yhat . Tree models . Randomforest . reg = RandomForestRegressor(max_depth=2, random_state=0) reg.fit(X_train, y_train) yhat = reg.predict(X_test) resultsDict[&#39;Randomforest&#39;] = evaluate(df_test.pollution_today, yhat) predictionsDict[&#39;Randomforest&#39;] = yhat . XGBoost . reg = xgb.XGBRegressor(objective =&#39;reg:squarederror&#39;,n_estimators=1000) reg.fit(X_train, y_train, verbose=False) # Change verbose to True if you want to see it train yhat = reg.predict(X_test) resultsDict[&#39;XGBoost&#39;] = evaluate(df_test.pollution_today, yhat) predictionsDict[&#39;XGBoost&#39;] = yhat . plt.plot(df_test.pollution_today.values , label=&#39;Original&#39;) plt.plot(yhat,color=&#39;red&#39;,label=&#39;XGboost&#39;) plt.legend() . &lt;matplotlib.legend.Legend at 0x7f82e05f8cc0&gt; . Lightgbm . A tree gradient boosting model by microsoft . lightGBM = lgb.LGBMRegressor() lightGBM.fit(X_train,y_train) yhat = lightGBM.predict(X_test) resultsDict[&#39;Lightgbm&#39;] = evaluate(df_test.pollution_today, yhat) predictionsDict[&#39;Lightgbm&#39;] = yhat . Support vector machines . Explain multiple kernels balbla . reg = svm.SVR(kernel=&#39;rbf&#39;, C=100, gamma=0.1, epsilon=.1) reg.fit(X_train, y_train) yhat = reg.predict(X_test) resultsDict[&#39;SVM RBF&#39;] = evaluate(df_test.pollution_today, yhat) predictionsDict[&#39;SVM RBF&#39;] = yhat . Nearest neighbors . reg = KNeighborsRegressor(n_neighbors=2) reg.fit(X_train, y_train) yhat = reg.predict(X_test) resultsDict[&#39;Kneighbors&#39;] = evaluate(df_test.pollution_today, yhat) predictionsDict[&#39;Kneighbors&#39;] = yhat . Prophet multivariate . prophet = Prophet( growth=&#39;linear&#39;, seasonality_mode=&#39;multiplicative&#39;, daily_seasonality=True, ).add_country_holidays(country_name=&#39;China&#39;) for col in prophet_training.columns: if col not in [&quot;ds&quot;, &quot;y&quot;]: prophet.add_regressor(col) . prophet.fit(prophet_training) yhat = prophet.predict(prophet_test) resultsDict[&#39;Prophet multivariate&#39;] = evaluate(y_test, yhat.yhat.values) predictionsDict[&#39;Prophet multivariate&#39;] = yhat.yhat.values . plt.plot(df_test.pollution_today.values , label=&#39;Original&#39;) plt.plot(yhat.yhat,color=&#39;red&#39;,label=&#39;Prophet multivariate&#39;) plt.legend() . &lt;matplotlib.legend.Legend at 0x7f82e030aef0&gt; . Deep learning . Tensorlfow LSTM . LSTM are a special type of neural network architecture, you can read more on this here . We will be trying a LSTM model for our benchmark but we will need to reshape our data to provide the network a window of previous samples (past days data) for each y target value. Find the code here . BATCH_SIZE = 64 BUFFER_SIZE = 100 WINDOW_LENGTH = 24 def window_data(X,Y,window=7): &#39;&#39;&#39; The dataset length will be reduced to guarante all samples have the window, so new length will be len(dataset)-window &#39;&#39;&#39; x=[] y=[] for i in range(window-1,len(X)): x.append(X[i-window+1:i+1]) y.append(Y[i]) return np.array(x), np.array(y) #Since we are doing sliding, we need to join the datasets again of train and test X_w = np.concatenate((X_train, X_test)) y_w = np.concatenate((y_train, y_test)) X_w,y_w = window_data(X_w,y_w,window=WINDOW_LENGTH) X_train_w = X_w[:-len(X_test)] y_train_w = y_w[:-len(X_test)] X_test_w = X_w[-len(X_test):] y_test_w = y_w[-len(X_test):] #Check we will have same test set as in the previous models, make sure we didnt screw up on the windowing print(f&quot;Test set equal: {np.array_equal(y_test_w,y_test)}&quot;) train_data = tf.data.Dataset.from_tensor_slices((X_train_w, y_train_w)) train_data = train_data.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat() val_data = tf.data.Dataset.from_tensor_slices((X_test_w, y_test_w)) val_data = val_data.batch(BATCH_SIZE).repeat() . Test set equal: True . dropout=0.0 simple_lstm_model = tf.keras.models.Sequential([ tf.keras.layers.LSTM(128, input_shape=X_train_w.shape[-2:],dropout=dropout), tf.keras.layers.Dense(128), tf.keras.layers.Dense(128), tf.keras.layers.Dense(1) ]) simple_lstm_model.compile(optimizer=&#39;rmsprop&#39;, loss=&#39;mae&#39;) # logdir = &quot;logs/scalars/&quot; + datetime.now().strftime(&quot;%Y%m%d-%H%M%S&quot;) #Support for tensorboard tracking! # tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir) . EVALUATION_INTERVAL = 200 EPOCHS = 5 model_history = simple_lstm_model.fit(train_data, epochs=EPOCHS, steps_per_epoch=EVALUATION_INTERVAL, validation_data=val_data, validation_steps=50)#,callbacks=[tensorboard_callback]) #Uncomment this line for tensorboard support . Train for 200 steps, validate for 50 steps Epoch 1/5 200/200 [==============================] - 5s 25ms/step - loss: 52.4208 - val_loss: 45.1430 Epoch 2/5 200/200 [==============================] - 3s 16ms/step - loss: 35.8295 - val_loss: 35.0000 Epoch 3/5 200/200 [==============================] - 3s 16ms/step - loss: 30.0660 - val_loss: 31.3743 Epoch 4/5 200/200 [==============================] - 3s 15ms/step - loss: 27.4224 - val_loss: 29.6516 Epoch 5/5 200/200 [==============================] - 3s 16ms/step - loss: 25.2445 - val_loss: 29.5529 . yhat = simple_lstm_model.predict(X_test_w).reshape(1,-1)[0] resultsDict[&#39;Tensorflow simple LSTM&#39;] = evaluate(y_test,yhat) predictionsDict[&#39;Tensorflow simple LSTM&#39;] = yhat . DeepAR . DeepAR is a deep learning architecture released by amazon . features = [&#39;dew&#39;, &#39;temp&#39;, &#39;press&#39;, &#39;wnd_spd&#39;, &#39;snow&#39;, &#39;rain&#39;, &#39;pollution_yesterday&#39;, &#39;hour&#39;, &#39;dayofweek&#39;, &#39;quarter&#39;, &#39;month&#39;, &#39;year&#39;, &#39;dayofyear&#39;, &#39;sin_day&#39;, &#39;cos_day&#39;, &#39;dayofmonth&#39;, &#39;weekofyear&#39;] scaler = StandardScaler() scaler.fit(X_train) #No cheating, never scale on the training+test! df_training[features] = scaler.transform(df_training[features]) df_test[features] = scaler.transform(df_test[features]) training_data = ListDataset( [{&quot;start&quot;: df_training.index[0], &quot;target&quot;: df_training.pollution_today, &#39;feat_dynamic_real&#39;: [df_training[feature] for feature in features] }], freq=&quot;d&quot; ) test_data = ListDataset( [{&quot;start&quot;: df_test.index[0], &quot;target&quot;: df_test.pollution_today, &#39;feat_dynamic_real&#39;: [df_test[feature] for feature in features] }], freq=&quot;d&quot; ) . estimator = DeepAREstimator(freq=&quot;d&quot;, prediction_length=1 , context_length=30, trainer=Trainer(epochs=5)) predictor = estimator.train(training_data=training_data) forecast_it, ts_it = make_evaluation_predictions(test_data, predictor=predictor, num_samples=len(df_test)) forecasts = list(forecast_it) tss = list(ts_it) . INFO:root:Using CPU INFO:root:Start model training INFO:root:Epoch[0] Learning rate is 0.001 0%| | 0/50 [00:00&lt;?, ?it/s]INFO:root:Number of parameters in DeepARTrainingNetwork: 25884 100%|██████████| 50/50 [00:01&lt;00:00, 30.25it/s, avg_epoch_loss=5.73] INFO:root:Epoch[0] Elapsed time 1.654 seconds INFO:root:Epoch[0] Evaluation metric &#39;epoch_loss&#39;=5.731202 INFO:root:Epoch[1] Learning rate is 0.001 100%|██████████| 50/50 [00:01&lt;00:00, 31.59it/s, avg_epoch_loss=5.54] INFO:root:Epoch[1] Elapsed time 1.584 seconds INFO:root:Epoch[1] Evaluation metric &#39;epoch_loss&#39;=5.539167 INFO:root:Epoch[2] Learning rate is 0.001 100%|██████████| 50/50 [00:01&lt;00:00, 31.62it/s, avg_epoch_loss=5.46] INFO:root:Epoch[2] Elapsed time 1.582 seconds INFO:root:Epoch[2] Evaluation metric &#39;epoch_loss&#39;=5.463747 INFO:root:Epoch[3] Learning rate is 0.001 100%|██████████| 50/50 [00:01&lt;00:00, 31.26it/s, avg_epoch_loss=5.37] INFO:root:Epoch[3] Elapsed time 1.601 seconds INFO:root:Epoch[3] Evaluation metric &#39;epoch_loss&#39;=5.374336 INFO:root:Epoch[4] Learning rate is 0.001 100%|██████████| 50/50 [00:01&lt;00:00, 30.09it/s, avg_epoch_loss=5.29] INFO:root:Epoch[4] Elapsed time 1.663 seconds INFO:root:Epoch[4] Evaluation metric &#39;epoch_loss&#39;=5.293175 INFO:root:Loading parameters from best epoch (4) INFO:root:Final loss: 5.293174734115601 (occurred at epoch 4) INFO:root:End model training . yhat = forecasts[0].samples.reshape(1,-1)[0] resultsDict[&#39;DeepAR&#39;] = evaluate(y_test,yhat) predictionsDict[&#39;DeepAR&#39;] = yhat . Appendix . Hyperparameter optimization . We have seen models with really low amount of parameters (Auto regression models,Linear models) or with crazy ammount (Trees,Prophet). Some models are more robust to different data types/shapes and dont need any hyperparameter optimization but some other can give you poor results if the parameters are not tunned, we can tune the model parameters to better fit our dataset properties. We can do this manually with pure knowledge about the model but this becames really hard when the model contains a lot of different parameters, this is when hyperparameter optimization comes handy. . Hyperparameter optimization is trying to find the best parameters in an automatic way. We present two methods that are used frequently: . Grid search Brute force method to try all different possible combinations of parameters. Will always find the best combination | Bayesian processes &quot;Brute&quot; force method, optimizes parameter search by using gausian processes to model each parameter distribution and don&#39;t go over all the possible values. Really nice library for python https://github.com/fmfn/BayesianOptimization, this method will not always find the best combination of parameters | . We provide 1 example for each method . Grid search - SVM . With grid search we can use the handy sklearn implementation . reg = GridSearchCV(svm.SVR(kernel=&#39;rbf&#39;, gamma=0.1), param_grid={&quot;C&quot;: [1e0, 1e1, 1e2, 1e3], &quot;gamma&quot;: np.logspace(-2, 2, 5)}) reg.fit(X_train, y_train) yhat = reg.predict(X_test) resultsDict[&#39;SVM RBF GRID SEARCH&#39;] = evaluate(df_test.pollution_today, yhat) predictionsDict[&#39;SVM RBF GRID SEARCH&#39;] = yhat . increase = 1 - (resultsDict[&#39;SVM RBF GRID SEARCH&#39;][&#39;rmse&#39;]/resultsDict[&#39;SVM RBF&#39;][&#39;rmse&#39;]) print(f&quot;Grid search Tunned SVM is {increase*100}% better than the SVM with default parameters&quot;) . Grid search Tunned SVM is 10.159715482199305% better than the SVM with default parameters . Bayesian processes - Xgboost . def rms(y_actual,y_predicted): return sqrt(mean_squared_error(y_actual, y_predicted)) my_scorer = make_scorer(rms, greater_is_better=False) pbounds = { &#39;n_estimators&#39;: (100, 10000), &#39;max_depth&#39;: (3,15), &#39;min_samples_leaf&#39;: (1,4), &#39;min_samples_split&#39;: (2,10), } def rf_hyper_param(n_estimators, max_depth, min_samples_leaf, min_samples_split): max_depth = int(max_depth) n_estimators = int(n_estimators) clf = RandomForestRegressor(n_estimators=n_estimators, max_depth=int(max_depth), min_samples_leaf=int(min_samples_leaf), min_samples_split=int(min_samples_split), n_jobs=1) return -np.mean(cross_val_score(clf, X_train, y_train, cv=3)) optimizer = BayesianOptimization( f=rf_hyper_param, pbounds=pbounds, random_state=1, ) . optimizer.maximize( init_points=3, n_iter=20, acq=&#39;ei&#39; ) . | iter | target | max_depth | min_sa... | min_sa... | n_esti... | - | 1 | -0.5467 | 8.004 | 3.161 | 2.001 | 3.093e+0 | | 2 | -0.5101 | 4.761 | 1.277 | 3.49 | 3.521e+0 | | 3 | -0.5456 | 7.761 | 2.616 | 5.354 | 6.884e+0 | | 4 | -0.5281 | 5.877 | 2.285 | 7.815 | 4.881e+0 | | 5 | -0.4625 | 3.0 | 1.0 | 2.0 | 1e+04 | | 6 | -0.46 | 3.315 | 3.21 | 7.499 | 3.52e+03 | | 7 | -0.5072 | 4.552 | 3.428 | 9.609 | 3.52e+03 | | 8 | -0.4606 | 3.054 | 3.6 | 9.411 | 3.428e+0 | | 9 | -0.5438 | 7.721 | 3.312 | 5.012 | 9.85e+03 | | 10 | -0.553 | 14.93 | 2.808 | 5.007 | 1.126e+0 | | 11 | -0.546 | 7.527 | 1.902 | 5.322 | 8.327e+0 | | 12 | -0.5424 | 7.21 | 2.036 | 9.914 | 105.4 | | 13 | -0.4626 | 3.309 | 2.022 | 6.58 | 5.884e+0 | | 14 | -0.5299 | 5.366 | 1.349 | 5.738 | 6.025e+0 | | 15 | -0.5294 | 5.541 | 2.695 | 4.973 | 5.742e+0 | | 16 | -0.5544 | 13.23 | 1.307 | 8.176 | 2.098e+0 | | 17 | -0.5545 | 14.55 | 1.474 | 3.641 | 9.09e+03 | | 18 | -0.4632 | 3.577 | 1.476 | 6.905 | 7.606e+0 | | 19 | -0.5437 | 7.3 | 3.928 | 4.117 | 7.746e+0 | | 20 | -0.4615 | 3.773 | 1.245 | 7.652 | 7.466e+0 | | 21 | -0.5396 | 6.043 | 2.401 | 6.272 | 7.319e+0 | | 22 | -0.5437 | 7.846 | 3.315 | 6.199 | 4.252e+0 | | 23 | -0.53 | 5.479 | 1.317 | 4.759 | 612.2 | ========================================================================= . params = optimizer.max[&#39;params&#39;] #Converting the max_depth and n_estimator values from float to int params[&#39;max_depth&#39;]= int(params[&#39;max_depth&#39;]) params[&#39;n_estimators&#39;]= int(params[&#39;n_estimators&#39;]) params[&#39;min_samples_leaf&#39;]= int(params[&#39;min_samples_leaf&#39;]) params[&#39;min_samples_split&#39;]= int(params[&#39;min_samples_split&#39;]) #Initialize an XGBRegressor with the tuned parameters and fit the training data tunned_rf = RandomForestRegressor(**params) tunned_rf.fit(X_train, y_train) # Change verbose to True if you want to see it train yhat = tunned_rf.predict(X_test) resultsDict[&#39;Randomforest tunned&#39;] = evaluate(y_test, yhat) . increase = 1 - (resultsDict[&#39;Randomforest tunned&#39;][&#39;rmse&#39;]/resultsDict[&#39;Randomforest&#39;][&#39;rmse&#39;]) print(f&quot;Bayesian optimized Randomforest is {increase*100}% better than the Randomforest with default parameters&quot;) . Bayesian optimized Randomforest is 10.593018169782265% better than the Randomforest with default parameters . Ensembling . Ensembling refers to combine multiple models to achieve a better performance, most of the time this only makes sense when models have similar performance but predict values differently so we try to get the best of each model. . We will pick our 3 top performing models and look at the correlation of their residuals, the less correlated the better . models = [&#39;Tensorflow simple LSTM&#39;, &#39;Lightgbm&#39;, &#39;XGBoost&#39;] resis = pd.DataFrame(data={k: df_test.pollution_today.values - v for k, v in predictionsDict.items()})[models] corr = resis.corr() print(&quot;Residuals correlation&quot;) corr.style.background_gradient(cmap=&#39;coolwarm&#39;) . Residuals correlation . Tensorflow simple LSTM Lightgbm XGBoost . Tensorflow simple LSTM 1 | 0.698879 | 0.660419 | . Lightgbm 0.698879 | 1 | 0.879884 | . XGBoost 0.660419 | 0.879884 | 1 | . We can see how both tree models are a bit similar ~0.87 but quite different from the Deep Learning model with corr ~0.7. In this case it would really make sense to ensemble the methods and see how they behave. The most reasonable combinations to try would be . XGboost + Tensorflow | XGBoost + Lightgbm | Lightgbm + Tensorflow | XGBoost + Lightgbm + Tensorflow | . We will just sum the predictions of each model with similar weights (0.5 if two models, 0.333 if three) . predictionsDict[&#39;EnsembleXG+LIGHT&#39;] = (predictionsDict[&#39;XGBoost&#39;] + predictionsDict[&#39;Lightgbm&#39;])/2 resultsDict[&#39;EnsembleXG+LIGHT&#39;] = evaluate(df_test.pollution_today.values,predictionsDict[&#39;EnsembleXG+LIGHT&#39;]) predictionsDict[&#39;EnsembleXG+LIGHT+TF&#39;] = (predictionsDict[&#39;XGBoost&#39;] + predictionsDict[&#39;Lightgbm&#39;]+ predictionsDict[&#39;Tensorflow simple LSTM&#39;])/3 resultsDict[&#39;EnsembleXG+LIGHT+TF&#39;] = evaluate(df_test.pollution_today.values,predictionsDict[&#39;EnsembleXG+LIGHT+TF&#39;]) predictionsDict[&#39;EnsembleLIGHT+TF&#39;] = (predictionsDict[&#39;Lightgbm&#39;]+ predictionsDict[&#39;Tensorflow simple LSTM&#39;])/2 resultsDict[&#39;EnsembleLIGHT+TF&#39;] = evaluate(df_test.pollution_today.values,predictionsDict[&#39;EnsembleLIGHT+TF&#39;]) predictionsDict[&#39;EnsembleXG+TF&#39;] = (predictionsDict[&#39;XGBoost&#39;]+ predictionsDict[&#39;Tensorflow simple LSTM&#39;])/2 resultsDict[&#39;EnsembleXG+TF&#39;] = evaluate(df_test.pollution_today.values,predictionsDict[&#39;EnsembleXG+TF&#39;]) . def full_extent(ax, pad=0.0): &quot;&quot;&quot;Get the full extent of an axes, including axes labels, tick labels, and titles.&quot;&quot;&quot; # For text objects, we need to draw the figure first, otherwise the extents # are undefined. ax.figure.canvas.draw() items = ax.get_xticklabels() + ax.get_yticklabels() # items += [ax, ax.title, ax.xaxis.label, ax.yaxis.label] items += [ax, ax.title] bbox = Bbox.union([item.get_window_extent() for item in items]) return bbox.expanded(1.0 + pad, 1.0 + pad) . from utils.plots import bar_metrics bar_metrics(resultsDict) . Looks like we get even better performance! . Feature importance . Some models allow for for native feature importance algorithms but I personally like the library SHAP that provides a game theory approach to measure how each feature affects our forecast. . Here is an example on how to use SHAP for our Lightgbm model . explainer = shap.TreeExplainer(lightGBM) shap_values = explainer.shap_values(X_train_df) shap.summary_plot(shap_values, X_train_df) . df = pd.DataFrame.from_dict(resultsDict).transpose().iloc[::-1] df.to_csv(&quot;results/results_summary.csv&quot;) . df.to_html(&quot;results/results_summary.html&quot;) . Possible improvements . Parameter tunned lightgbm and xgboost and redo the ensemble with Tensorflow | . Additional resources and literature . . Adhikari, R., &amp; Agrawal, R. K. (2013). An introductory study on time series modeling and forecasting | [1] | . Introduction to Time Series Forecasting With Python | [2] | . Deep Learning for Time Series Forecasting | [3] | . The Complete Guide to Time Series Analysis and Forecasting | [4] | . How to Decompose Time Series Data into Trend and Seasonality | [5] | .",
            "url": "https://jithendray.github.io/blogy/2021/01/01/time-series-forecasting-tutorial.html",
            "relUrl": "/2021/01/01/time-series-forecasting-tutorial.html",
            "date": " • Jan 1, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Understanding Time Series and ARIMA",
            "content": "In a lot of machine learning problems the thing we want to predict is dependent on very clear inputs, such as properties of the target, pixels of an image, etc. In time series these indepent variables are often not known or does not exist. . For example, in predicting the demand of fresh vegetable products from a market, we don’t have a clear independent set of variables where we can fit a model on. A market collects vegetables from various farmers of different places. Is demand of a vegtable product dependent on the properties of a farm it was grown in, or the temperature at that place or the height and weight of the farmer? No, the demand of a vegetable does not depend on the farm or the farmer. The independent data is not easily available or even if we can try to find a relationship between such independent variables and the vegetable demand, these relationships are not perfect and clear. . The time series analysis is frequently used in such cases. The fundamental intuition behind time series forecasting is that, the measure of some variable at a time period will depend on the measure of the same variable at previous time periods. Therefore, we analyze the series on the series itself. If you are walking the path of becoming a data scientist, you might have already come across the term Time Series and you might have also realized the importance of Time Series Analysis and Forecating. In this post, I will try to give a gentle introduction so that it can kickstart your learning. . . What is Time series? . As the name suggests, time series is just a series of observations collected over different points in time. There exists a correlation between the observations collected at adjacent time points, therefore the previous observations of a variable can be used in predicting the same variable. This distinguishes time series data from general machine learning data where the observtions are collected at a single point in time. . The data collected over time represents a time series only if the observations are dependent on time. If the data collected is purely random in nature, forecating the future values is not possible and such data is called white noise.  . Univariate vs Multivariate time series . If only a single variable is varying over time, it is called Univariate time series. For example, temperature of a room measured every hour. Here there are no other variables recorded, hence predicting temperature only depends on the temperature values recorded at previous time points. . If there are more than one variable varying over time, it is called Multivariate time series. For example, if the humidity is recorded along with the temperature then both temperature and humidity are to be considered in order to predict the temperature. . Note: Predecting the future is not the only goal of time series data. We can have different goals while working with time series. These goals can be mainly categorized into analysis and forecasting. . Time Series Analysis . The main goal of time series analysis is exctracting useful statistics from data in order to understand the nature and underlying causes of the past. It helps to describe available data and provide interpretation to understand the problem domain better. Time series analyis can help to make better predictions. . Time Series Forecasting . The main goal of forecasting is to build models on the past data and use them to predict future observations. For example, predicting number of births in a country based on the data collected in past years. This is challenging as the future observations are unavailable and must be predicted from what has already happened in the past. . Stationarity in Time Series . If all the statistical characteristics of data like mean, auto correlation, variance do not vary with time then the time series is called stationary. But in general, most of the data recorded in not stationary i.e. the properties vary with time. . Analyzing such time series helps to understand the patterns such as trend, seasonality,cyclicality and irregularity. Trend is a general direction the data is changing as time passes. Seasonality is when a pattern recurs over fixed regular time intervals. Cyclicality is when there are any fluctuations around the trend. Unlike seasonality, cyclicality may vary in length. Irregularity is when there are random fluctuations which are not systematic and are irregular. These fluctuations cannot be controlled. These are called as time series components. . . Most of the forecasting methods assume that the data is stationary because it is easy to predict the stationary data. Therfore, it is important to convert non-stationary data to stationary in order to apply forecasting models. . Check out this post by Mehul Gupta which explains Why time series has to be stationary more clearly. . It is important to analyze these components carefully in order to better understand the problem during analysis or forecasting. Since it is difficult to see all the components in a time series, a method called Decomposition can be used to identify them. These components can either combine in an additive way or in a multiplicative way. . An Additive time series is when the fluctuations in the data do not vary over time. Additive model is linear and seasonality has same frequency although the time increases. . Time series = trend + seasonality + cyclicality + irreguarity . A Multiplicative time series is when the variations or the fluctuations in the data increases as the time increases. Multiplicative model is non-linear and seasonality has either increasing or decreasing frequency. . Time series = trend * seasonality * cyclicality * irregularity . Time Series Decomposition . The purpose of decomposition is to identify and seperate components from a time series in order to perform better analysis and forecasting. . In general, the cyclical component is hard to seperate and it is left by grouping it with the trend component, to form a trend-cycle component. It is often simply referred to as the trend component, even though it may contain cyclical behavior. . Classical decomposition can be either a multiplicative or an additive decomposition. A function called seasonal_decompose() can be used to perform classical decomposition. You need to mention whether the model is additive or multiplicative. . Below is an example which shows decomposition of a dataset into components including the original data, trend, seasonality and irregularity(residual). . Let’s first load the dataset and plot a simple graph: . import pandas as pd import matplotlib.pyplot as plt df = pd.read_csv(&#39;births.csv&#39;) df.plot() plt.show() . . Since the variations are very complex, we cannot see all the components clearly. Now, Decomposing this will give us clear picture of the components. Let’s look how we can decompose this using seasonal_decompose() function: . from statsmodels.tsa.seasonal import seasonal_decompose components = seasonal_decompose(df[&#39;births&#39;], model=&#39;multiplicative&#39;,period=10) components.plot() pyplot.show() . . Now you can see all the components, you can analyze them and remove any of them them if not needed. For example, if you want to analyze the trend of a stock data, you would need to remove the seasonality found in the data and the noise due to irregularity. . Let’s look into the basic steps to be followed while performing a forecasting task - . Basic steps for Forecasting . Defining the problem: Understanding the problem domain and clearly knowing the end goal of the forecast. The most important skill needed for a data scientist is being able to explain why a prediction is made and present results in a proper way. This is possible only with having a clear knowledge of who needs the forecast, why and how it will be used. | Data Collection: Collecting the past data related to the problem domain, gathering other important information from domain experts. | Data preperation: This includes exploring the data to know components like trend or seasonality, cleaning the data to fill the missing values and remove outliers if any, basic feature engineering to understand the relation ship between features or to add any new features, resampling and data transforms to remove noise and improve the forecasting. | Modeling: This includes configuring the right forecast model for the data. Widely used time series models are Auto Regressive(AR) models, Moving Average(MA) models, Intergrated(I) models and the combination of these models like Auto Regressive Moving Average models(ARMA), Auto Regressive Integrated Moving Average models(ARIMA). It is better to try models of different types, from simple to advanced approaches. | Evaluation: The time series forecasting model can only be trusted through its performance at predicting the future. This may include testing the model on previous data by creating train-test splits and calculating error or wait for the new observations to occur to compare the predictions. | Applications of Time Series Forecasting . Forecasting of agricultural commodity price | Stock market analysis and forecasting | Sales forecasting | Forecasting supply chain components | Weather forecasting | Forecasting the birth rate in a country | . any many more ….. . AR: Auto Regressive model . Auto Regressive (AR) model is a specific type of regression model where, the dependent variable depends on past values of itself. . For example, consider a stock price data. The stock price of today is influenced by yesterday’s price. If the price(close price) of day t-1 is x dollars, the price of day t starts with x dollars. We can assume the price can be determined by the following model: . $Y_{t} = mu + phi Y_{t-1} + epsilon_t$ . where $ mu$ and $ phi$ are constants, and $ epsilon_t$ is white noise. . This model is called AR model, and generally AR(p) is given by the following definition: . $Y_t = mu + epsilon_t + phi_1Y_{t-1} + phi_2Y_{t-2} + …+ phi_pY_{t-p}$ . MA: Moving Average model . Moving Average (MA) model works by analysing how wrong you were in predicting values for the previous time-periods to make a better estimate for the current time-period. This model factors in errors from the observations. . Generally MA(q) is given by the following definition: . $Y_t = mu + epsilon_t + theta_1 epsilon_{t-1} + theta_2 epsilon_{t-2} +…+ theta_q epsilon_{t-q}$ . MA model complements the AR model by taking the errors from the previous time-periods into considerations, to get better forecast results. . The combined model between AR and MA is called ARMA model, and it is defined as AR(p,q). It is given as follows: . $Y_t = mu+ epsilon_t+ phi_1Y_{t-1}+..+ phi_pY_{t-p}+ theta_1 epsilon_{t-1}+..+ theta_q epsilon_{t-q}$ . Estimating the ARMA(p,q) model hyperparameters . When selecting the parameters p and q, we must focus on characteristics of the model. Each model have different characteristics for autocorrelation function (ACF) and partial autocorrelation function (PACF). By looking at the ACF and PACF plots, we can identify the numbers of MA and AR terms i.e. q and p respectively. . Autocorrelation . The relationship between two variables is summarized by correlation. If the correlation is calculated for the timeseries observations with the observations at previous time steps is called as Autocorrelation. The observations of previous timesteps are called as lags. The ACF plot is a bar chart of the coefficients of correlation between timeseries and lags of itself. . Partial Autocorrelation . The “partial” correlation between two variables is the amount of correlation between them which is not explained by their mutual correlations with a specified set of other variables. If the correlation is calculated for the timeseries observations with the observations at previous time steps at lag k by eliminating all the effects of shorter lags i.e. 1,2,…k-1, then it is called as Partial Autocorrelation. The PACF plot is a plot of the partial correlation coefficients between the series and lags of itself. . Characteristics of ACF and PACF . Now, the models have the following characteristics for the autocorrelation and partial autocorrelation. . AR(p): When the lag is getting large, The autocorrelation decreases exponentially and the partial autocorrelation cuts-off after lag p. | MA(q): When the lag is getting large, the autocorrelation cuts-off after lag q and the partial autocorelation tails off to zero. | ARMA(p,q): When the lag is getting large, both autocorrelation and partial autocorrelation tails off to zero. | . Using these characteristics, we can estimate the proper model. . Stationarity . AR, MA and ARMA models require the data to be stationary. A stationary series has a constant mean and variance over time. But in real-world, most of the data is not stationary. . So how to forecast non-stationary series? . Well, here comes the ARIMA model which works with data that is not stationary. The new term added for ARIMA is I. . Integrated (I) . Consider a non-stationary series which needs to be forecasted. We can say whether the data is stationary or not by studying the plot against time. . . It is clearly visible that the mean is increasing over time i.e. the series is not stationary. If this upward trend is eliminated, the series becomes stationary. The easiest way to do this is to consoder the differences between consecutive timesteps. It goes as follows: . $I_t = Y_{t+1} - Y_t$ . After applying this transformation, the series becomes like this with observable linear trend - . . The transformed series is called as the differenced series. This differenced series is used for forecasting instead of the timeseries. This step of converting non-stationary series to stationary series results in the new term I which stands for Integrated. (Note: This has nothing to with integration). . In the above example, the data becomes stationary after performing the first order differencing which means a single time differencing of observations at consecutive timesteps. But in some cases, the data remains non-stationary after performing the first order differencing. Hence, the series could be differenced more than once to make it stationary. . d = 1: $I_t = Y_{t+1} - Y_t$ . d = 2: $J_t = I_{t+1} - I_t =&gt; Y_{t+2}+Y_t-2Y_{t+1}$ . Hence, ARIMA has hyperparameters p, q and d: . p - order of the AR model | q - order of the MA model | d - order of differencing | . Seasonality . It is to be noted that ARIMA model assumes that the data is not seasonal. The presence of variations that occur at specific regular intervals, such as weekly or monthly results in seasonality of time series. For example, Arrival of vegetables to the market or sales of Diwali crackers. ARIMA does not work well for seasonal series. . There is an upgrade of ARIMA model, called Seasonal ARIMA or SARIMA. It is represented as ARIMA (p,d,q) X (P,D,Q,m). . p - trend AR order | q - trend MA order | d - trend difference order | P - seasonal AR order | Q - seasonal MA order | D - seasonal difference order | m - frequency of seasonality in timeseries | . To identify values for the seasonal model, ACF and PACF plots can be analyzed by looking at correlation at seasonal lag time steps i.e. the lag at which seasonality occurs. Alternately, this post explains how grid searching can be used across the trend and seasonal hyperparameters. . Another easy way to use is to use pmdarima’s auto.arima which automatically gives the best fit model. . Further Reading . Fundamentals of Time series Data and Analysis | ARIMA models for time series forecasting | Summary of rules for identifying ARIMA models | Notes on nonseasonal ARIMA models | How does auto.arima() work? | . Credits . Thanks for reading! And please feel free to connect to me via twitter or linkedin. Feedback is always welcome. .",
            "url": "https://jithendray.github.io/blogy/datascience/timeseries/2020/11/11/ts-arima.html",
            "relUrl": "/datascience/timeseries/2020/11/11/ts-arima.html",
            "date": " • Nov 11, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Understanding Generative Adversarial Networks",
            "content": "When you create something out of nothing, it is the most thrilling thing — Frankie Knuckles, American DJ . Generative Adversarial Networks are exactly the same thrilling thing. They create some new data out of nothing, following the rules established by existing data. This ability to generate information out of nothing makes GANs look like a bit magical. And the results that are generated are very promising. Yann LeCun — one of the most prominent researchers in deep learning described it as “ the most interesting idea in the last 10 years in Machine Learning”. And indeed GANs have had a huge success and thousands of research papers were published in the recent years. . . The main motto of GAN is to generate some information from scratch. But their potential is very huge. Just to give you the idea of their potential, I am mentioning some of the coolest projects created with GANs that you should definitely check out: . . . . So what are Generative Adversarial Networks? What is so magical about them? In this blog post we’ll explore GANs and detailed explanation of how GANs work. But before diving into GANs, we will start by describing what are Generative models. . What is a Generative model? . To understand what a generative model is, contrasting it with a discriminative model is helpful. Discriminative model discriminates between different kinds of data instances whereas a Generative model generates a new data instance. Given the features of a data instance, discriminative model predicts a category to which that data belongs whereas a generative model do the opposite. Instead of predicting a label based on features, a generative model predicts features based on labels. It cares about the distribution of the training data. A Generative Model is a unsupervised learning method that learns any kind of data distribution and it has achieved huge success in the past few years. . Types of Generative models: . . Generative models are of two types — explicit density models and implicit density models. The main difference between them is that explicit models use an explicit density function whereas the implicit models use a stochastic procedure that can directly generate data. . Explicit density models . Explicit density models are again divided into Tractable density and Approximate density models. In general, tractable distribution means it takes polynomial-time to capture probability of its distribution at any given point. Pixel RNN is the most commonly used tractable density model. They are highly effective but they follow sequential generation, which is very slow. . But most of the distributions are complex and it is very difficult to capture the distribution in polynomial time. Such models are considered as Approximate models. These are again divided into two categories: models using variational methods and models using Monte Carlo methods. Variational methods use deterministic approximations and are used in complex models with unknown parameters. Variational Autoencoder (VAE) is one of the most popular generative models and is based on variational learning. Boltzmann machines are another kind of generative models that rely on Markov chains. They use stochastic approximations instead of deterministic approach. Boltzmann machines played an important part in deep learning research but now they are very rarely used because Markov chains inflict very high computational costs. . Implicit density models . These can be trained without explicitly defining a density function. There are some implicit models too which rely on Markov chains like Generative Stochastic Network (GSN) but as we already discussed that Markov chains inflict high computational costs, they fail in many cases. Generative Adversarial Networks were designed to avoid most of these disadvantages associated with other generative models. GANs have become so popular because they are proven to be really successful in modeling and generating high dimensional data. . Enough of this background knowledge. Though it is a very captivating field to explore and discuss, I’ll try to leave the in-depth explanation later in another post, we are here for GANs! Now without wasting any further time, let’s understand how do GANs work !!! . Understanding a GAN . The end goal of a generative model is to predict features given a label, instead of predicting a label given features. They try to learn the model to generate the input distribution as realistic as possible. So the main focus here is to design an architecture of a network that takes a simple N dimensional uniform random variable as input and returns another N dimensional random variable that should somehow follow the probability distribution of input sample as output. Now we need to optimize the network through training. In general, the generated random distribution is directly compared to the input sample and use back propagation to lower the distance between true and generated samples. . Ian Goodfellow and his colleagues came up with a brilliant idea in the 2014 paper titled “Generative Adversarial Nets”. They proposed a new framework in which two neural networks compete with each other for estimating generative models. In the following sections we will understand the training process and the math behind GANs. . The GAN architecture: . . A generative neural network is composed of two models: the Generator which generates data from some random uniform distribution and the Discriminator which identifies the fake data from the real data . The output of generator(fake data) is connected to the discriminator input. . The simplest way to understand the architecture is — a generator network trained to generate samples as realistic as possible via adversarial training by introducing a discriminator network, which plays a role of detecting whether the given sample is real or fake. The generator should learn to fool the discriminator into believing that the input sent by generator is real. While the discriminator tries not to get fooled by generator identifying that the data generated is fake. These two models improves their knowledge by competing with each other, until generator wins in fooling the discriminator. . Since we got an overview of GAN architecture, we will now understand how these models compete with each other technically! . Training a GAN: . We define a neural network G(z, θg) that maps random noise variables z to some data x. We also define another neural network D(x, θd) that outputs a single scalar D(x) that represents the probability that the input came from the real dataset. θg and θd represents parameters that define respective neural networks. The generator and the discriminator have two separate training processes. . The discriminator is simply a classifier. We can use any architecture for the discriminator that is appropriate to the type of data we are using. The discriminator is trained in such a way that it classifies the input data as either real or fake. So the parameters (θd) of the discriminator are updated in order to minimize the probability that any fake data sample G(z) is classified as a real one and also to maximize the probability any real data sample x is classified as belonging to the real dataset. In order words, the loss function of discriminator minimizes D(G(z)) and maximizes D(x). Minimizing log(D(G(z))) is same as maximizing log(1-D(G(z))). So the objective for the discriminator becomes: . . The generator learns to make the discriminator classify the data generated as real through feedback from the discriminator. The parameters (θg) of the generator are updated in order to maximize the probability that any fake data sample is classified as a real one. So the loss function of generator maximizes D(G(z)). . . As Ian Goodfellow said, it is essentially two-player minimax game played by generator(G) and discriminator(D). The value function V(G, D) is given by: . . where — . D(x) : probability that the real data instance x is real . G(z) : generator’s output for noise z . D(G(z)) : probability that the generated instance is real . 1-D(G(z)) : probability that the generated instance is fake . A gradient-based optimization algorithm can be used to train the GAN since both the models are neural networks. We perform back propagation which allows the discriminator and generator to improve over time. Based on the classification done by the discriminator we will either have positive or negative feedback in the form of loss gradients. We keep the parameters of generator constant and train the discriminator during which it has to learn how to slap the generator’s flaws. Then we switch the models. We keep the parameters of the discriminator constant and train the generator. . In this way, we train both the networks alternatively and the networks will compete with each other to improve themselves. Eventually the generator generates realistic data and the discriminator will be unable to find the difference between the real data sample and the generated data sample. . I am adding a screenshot from the paper which explains the algorithm on how to train a GAN using stochastic gradient descent. . . The training steps for a GAN can be described like this: . From a random distribution we take some noise and send it to the generator G which produces some generated fake data. | Along with the generated data, we also send the sample of real data to the discriminator D. | The discriminator calculates the loss for both the real and fake data samples and the generator also calculates the loss from the noise. | The two calculated losses are back propagated to their respective networks and the networks learn to improve from these losses. | Apply optimization algorithm like gradient descent and Repeat the whole process. | . “Talk is cheap, Show me the code” — Linus Torvalds . Okay, We’ll now implement a GAN to understand this better. . Implementing a GAN . We are going to implement a GAN using PyTorch. We’ll start by creating a notebook and importing the following dependencies. . import torch import torch.optim as opt import torch.nn as nn import torch.nn.functional as F import torchvision import torchvision.transforms as transforms import numpy as np . Dataset . The Dataset we will be using is the classic MNIST dataset created by LeCunn. The dataset consists of 60,000 images of handwritten digits, each with size 28x28 . transform = transforms.ToTensor() # load data trainData = torchvision.datasets.MNIST(&#39;./data/&#39;, download=True, transform=transform, train=True) # creating a loader with data - which helps to iterate over the data batch_size = 64 trainLoader = torch.utils.data.DataLoader(trainData, shuffle=True, batch_size=batch_size . . Discriminator . This network will take an image as its input and return the probability of it belonging to the real dataset or the generated dataset. The input size for each image will be 28x28=784 . The architecture we are going to implement will have three Fully-connected layers, each followed by ReLU non-linearity layer. Since the output should be the probability of the image that says whether it is real or fake, the value should be between (0,1). For this purpose, a Sigmoid function is added to the real-valued output in the last layer. . class Discriminator(nn.Module): def __init__(self): super().__init__() self.model = nn.Sequential( nn.Linear(X_dim, 256), nn.LeakyReLU(inplace=True), nn.Linear(256,128), nn.LeakyReLU(inplace=True), nn.Linear(128, 1), nn.Sigmoid() ) def forward(self, input): return self.model(input) . Generator . The Generator network will take a random noise vector as input and returns a vector of size 784, which resembles a 28x28 image. The last layer will have Tanh activation to clip the image to be [-1,1] — which is same size as the preprocessed MNIST images are bounded. . class Generator(nn.Module): def __init__(self): super().__init__() self.model = nn.Sequential( nn.Linear(n_features, 128), nn.LeakyReLU(inplace=True), nn.Linear(128, 256), nn.LeakyReLU(inplace=True), nn.Linear(256, X_dim), nn.Tanh() ) def forward(self, input): return self.model(input) . Optimization . We’ll use Adam as the optimization algorithm with the learning rate 2e-4 which is not necessarily the optimal value, but selected after various tests. . g_opt = opt.Adam(G.parameters(), lr=2e-4) d_opt = opt.Adam(D.parameters(), lr=2e-4) . Training: . In the above section, we’ve already seen the steps that must be followed to train a GAN. First, we need to calculate losses of both generator and discriminator networks and then back-propagate them. . Discriminator loss function: . . Generator loss function: . . We will be using Binary Cross Entropy Loss or log loss because it resembles both the generator and the discriminator losses. . . For training discriminator, if we replace ŷi with D(x) and yi = 1 we will get the real image loss and if we replace ŷi with D(G(z)) and yi = 0 we will get fake image loss. We will add this together to get the total discriminator loss. . For training generator, we need to minimize log(1 - D(G(z))) which is same as maximizing log(D(G(z))). If we replace ŷi with D(G(z)) and yi = 1, we will get the loss to be maximized. But the problem with most of the frameworks like PyTorch is — they minimize the functions. Since, BCE-loss definition has a minus-sign, this won’t cause us any problem. . We will also create the real-image targets as ones, and the fake-image targets as zeros with shape (batch_size, 1). These will be help us in calculating the losses of generator and discriminator. . Training loop: . for epoch in range(20): G_loss_update = 0.0 D_loss_update = 0.0 for i, data in enumerate(trainLoader): X, _ = data X = X.view(X.size(0), -1) batch_size = X.size(0) # tensor containing ones representing real data target real_target = torch.ones(batch_size, 1) # tensor containing zeroes representing generated data target generated_target = torch.zeros(batch_size, 1) z = torch.randn(batch_size, n_features) # 1. Train discriminator # on real data D_real = D(X) # calculating real data error D_real_loss = F.binary_cross_entropy(D_real, real_target) # on generated data D_generated = D(G(z)) # calculating generated data error D_generated_loss = F.binary_cross_entropy(D_generated, generated_target) # Total discriminator loss D_loss = D_real_loss + D_generated_loss # reset gradients d_opt.zero_grad() # backpropagate D_loss.backward() # update weights with gradients d_opt.step() # 2. Train generator # sample noise and generate some fake data z = torch.randn(batch_size, n_features) D_generated = D(G(z)) # calculating error G_loss = F.binary_cross_entropy(D_generated, real_target) # resetting gradients, backpropagating and updating weights g_opt.zero_grad() G_loss.backward() g_opt.step() G_loss_update = G_loss_update + G_loss.item() D_loss_update = D_loss_update + D_loss.item() print(&#39;Epoch:{}, G_loss:{}, D_loss:{}&#39;.format(epoch, G_loss_update/(i+1), D_loss_update/(i+1))) samples = G(z).detach() samples = samples.view(samples.size(0), 1, 28, 28).cpu() imshow(samples) . We have successfully implemented a GAN. Now, let’s look at the results - . Before training: . . During training at 10th epoch: . . Finally: . . You can check out the complete implementation and run it online — kaggle notebook . . References: . [1] Goodfellow, Ian, et al. “Generative Adversarial Networks” NIPS, 2014. . [2] Uddin, S. M. Nadim. (2019). Intuitive Approach to Understand the Mathematics Behind GAN. 10.13140/RG.2.2.12650.36805. . [3] Ian Goodfellow’s NIPS 2016 tutorial on YouTube. .",
            "url": "https://jithendray.github.io/blogy/deeplearning/machinelearning/2020/07/11/Understanding-Generative-Adversarial-Networks.html",
            "relUrl": "/deeplearning/machinelearning/2020/07/11/Understanding-Generative-Adversarial-Networks.html",
            "date": " • Jul 11, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Upgrading Ubuntu to 20.04 LTS",
            "content": "I am very excited about the new version of Ubuntu. The beta version was released a month ago. But since it was fairly stable, I didn’t installed it. I killed my excitement and waited for the LTS version. And finally, on April 23rd Ubuntu 20.04 LTS was released. In this post, I’m going to share the process I used to upgrade my Ubuntu from 19.10 to 20.04. . Note: This is only when you are already running some ubuntu version and want to upgrade it to the new version. This is not the process for fresh installation of Ubuntu. . If you want to upgrade the Ubuntu version, you do need to download any .iso file or boot any USB drive. All you need is: . A good internet connection | And a bit of patience | . Note: You can upgrade from either Ubuntu 18.04 or Ubuntu 19.10 to 20.04LTS. But, If you are running the 16.04 version you will need to upgrade to 18.04 first and then to 20.04 LTS. . Another Note: Backup of important data is always required while performing a major upgrade. (But, I forgot to backup my data and luckily all the data is safe.) . This can be done using Ubuntu’s built-in do release upgrade tool which is also an easy way to upgrade. But I upgraded manually by updating the sources list with the guidance of Nerd on the street. The reason for upgrading manually is simply to enjoy the essence of linux. . Alright, here are the steps I followed to upgrade - . Step 1: Open the terminal and type the following commands . jithendra@hp:~$ cd /etc/apt . jithendra@hp:/etc/apt$ ls . The sources.list file tells ubuntu to what repositories to check in addition to any files in sources.list.d that end in the former one. In order to manually update, both of these should be updated. I started with sources.list . Step 2: Update ‘sources.list’ . jithendra@hp:/etc/apt$ sudo gedit sources.list . Step 3: On active lines, replace the codename of previous Ubuntu version with codename of Ubuntu 20.04 . The codename for Ubuntu 19.10 is eano and for Ubuntu 18.10, it is bionic. | Go through all active lines and replace these with focal. | Note: Active lines are the lines that are not starting with ‘#’. | . | The codename for Ubuntu 20.04 is Focal fossa and the keyword is focal. | Since I was using Ubuntu 19.10, I changed the keyword from eano to focal. | If the version is 18.04, then change from bionic to focal. | If the version is 19.04, then change it from disco to focal. | Save the file | . After updating sources.list file, I went back and check sources.list.d to see any 3rd party apps and ppa’s that also need to be upgraded. For example, google-chrome, spotify, and ppa’s that I might have installed. But, I dont want any third party applications and ppa’s in the new version. So I simply removed all the existing ppas from source.list.d using this command . sudo rm /etc/apt/sources.list.d/* . But, If you want your ppa’s and apps do the following . Step 4: Come back and verify ‘sources.list.d’ . jithendra@hp:/etc/apt$ cd sources.list.d/ . jithendra@hp:/etc/apt/sources.list.d$ ls . This outputs files like google-chrome.list, spotify.list, or ppa’s. | Third party apps dont use any codename in their repository. But this is not case for all third party repositories like ppa’s(personal package archives) hosted on launchpad. | If the repositories uses codenames, then gedit them manually in the same way I did for sources.list | Google-chrome, Spotify and many other third-party applications use stable codename in their repositories. There is no need in updating these codenames. Even some apps use xenial in codenames which need not be updated. | . Step 5: Update and Upgrade . jithendra@hp:/etc/apt/sources.list.d$ cd . jithendra@hp:~$ sudo apt update . I got a whole lot of stuff and all the gits of the updates are pointed out to focal. | This is just telling apt manually to look into focal and not into eano.(in my case) | In my case, I got like this 1873 packages can be upgraded. Run ‘apt list –upgradable’ to see them.` | This means 1873 packages are upgradable which are new. | . jithendra@hp:~$ sudo apt dist-upgrade . or . jithendra@hp:~$ sudo apt full-upgrade . There will be four kinds of packages in here The following packages were automatically installed and are no longer required (Use ‘sudo apt autoremove’ to remove them) | The following packages will be REMOVED | The following NEW packages will be installed | The following packages have been kept back | The following packages will be upgraded | . | Press yes and it will download all the packages and they will be unpacked one by one. | A question will be asked whether to restart servieces during package upgrades without asking - Select YES | Another question will be asked after sometime, whether I want to install package maintainer’s version of the configuration file or user currently installed version - select package manager’s version - select Y | ALMOST DONE | . Step 6: Remove the older clutter . jithendra@hp:~$ sudo apt autoremove –purge . purge removes configuration files too | . Step 7: REBOOT when everything is completed . When I start using my laptop then I faced an issue. | Because the new versions are on the harddrive. | To load all of that - | . jithendra@hp:~$ sudo systemctl reboot . Without any errors, My laptop was succesfully upgraded to Ubuntu 20.04 LTS. . Thank you for reading.Hope you find this post helpful. .",
            "url": "https://jithendray.github.io/blogy/linux/2020/05/01/upgrading-ubuntu.html",
            "relUrl": "/linux/2020/05/01/upgrading-ubuntu.html",
            "date": " • May 1, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://jithendray.github.io/blogy/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I’m a final year CS undergrad at Indian Institute of Information Technology, Jabalpur passionate about data science and machine learning research. My research interests include Machine Learning and Time series forecasting. . Currently, I’m interning as a Data Scientist @ NeenOpal, which is a global management consulting firm with a unique and specialized focus on Data Science based out in Banglore. . In the past, I’ve taken an interest in high performance deep learning on FPGA’s and statistical modeling for time series. During my undergraduation I designed a Neural Network accelerator on Intel CycloneV SoC FPGA using High Level Synthesis tools and languages. I also worked on forecasting project under Prof. Sunil Agrawal where I forecasted Onion arrival to Mumbai (a city in India) markets from farmers. . In the summer of 2019, I was a visiting researcher at HCI Lab, IIIT Allahabad working on early detection of Autism in toddlers using Machine Learning and Computer Vision. I was fortunate enough to work under the amazing mentorship of Prof. Anupam Agrawal. . I actively publish code to GitHub. I’m also active on Twitter and LinkedIn. If you think me (or my projects) are interesting feel free to contact me! You can also mail me at jithendra1230@gmail.com. I’m always interested in hearing about new opportunities to collaborate and work on machine learning projects or hackathons. . I spend my time analysing some publicly available data, reading research papers and writing. I spend my liesure time mostly banging my head to some heavy metal and some times I read books. Unfortunately, 2020 compelled me to blog about my weird music taste. I started writing my personal thoughts and impressions on Metal bands and their albums on Medium. . “Writing isn’t about making money, getting famous, getting dates, getting laid, or making friends. In the end it’s about enriching the lives of those who will read your work, and enriching your own life as well. It’s about getting up, getting well, and getting over. Getting happy, okay?” – Stephen King, On Writing: A Memoir of the Craft. . Links: . Twitter | LinkedIn | GitHub | .",
          "url": "https://jithendray.github.io/blogy/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://jithendray.github.io/blogy/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}