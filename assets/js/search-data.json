{
  
    
        "post0": {
            "title": "Weight Initialization",
            "content": "In this article, I am writing some notes about weight initialization. I took these from various sources I am reading regarding this. Links and References at the end of the post! Consider reading them to get a clear understanding! . NOTE: This is post for my future self to look back and review the material. So, this’ll be very unpolished! . Introduction . The first step that comes in consideration while building a neural network is the initialization of parameters - weights and biases. If not done correctly then layer activations might explode or vanish during the forward propagation which in turn makes loss gradients to be either too large or too small. Then achieving optimization will take longer or sometimes converging to a minima using gradient descent will be impossible. . Some key points to remember . If the weights are initialized too large or too small, the network won’t learn well - because it leads to exploding or vanishing gradients problem. | All weights should not be initialized with zeros. If neurons starts with same weights, then all neurons will learn the same features and perform the same thing as one another. | Neural Networks try to reach the local minima, If all the weights start at zero - it is not possible. So, it is better to give them different starting values. | . | . Weight Initialization methods . Normal Initialization . The authors of the famous Alexnet Paper initialized weights using zero-mean Guassian (normal) distribution with a standard deviation of 0.01. The biases were initialized as 1 for some layers and 0 for the rest. . Uniform initialization: bounded uniformly between ~ [(-1/√fin), (1/√fout)] . But this normal random initialization of weights does not work well for training deep neural networks, because of vanishing and exploding gradient problem. . Xavier Initialization / Glorot initialization [paper] . Proposed by Xavier and Bengio | considers number of input and output units while initializing weights | weights stay within a reasonable range by making them inversely proportional to the square root of the number of units in the previous layer | . Uniform: bounded uniformly between ~ [+/- sqrt(6/fin+fout) ] . Normal: multiply normal distribution by sqrt(2/fin+fout) . np.random.rand(shape) * np.sqrt(2/fin+fout) | or create normal distribution with μ = 0 and σ2 = sqrt(2/fin+fout) | . He initialization / Kaiming initialization paper . RELU activations are mostly used - bercause they are robust to vanishing/ exploding gradients. | A more robust initialization technique was introduced by Kaiming et al. for activation functions like RELU. | both Xavier and He use similar theory → find a good variance for the distribution from which the initial parameters are drawn | This variance is adapted to the activation function used | derived without explicitly considering the type of the distribution . . | Red → He and Blue → Xavier | . Uniform: [+/- √(6/fin)] Normal: normal distribution * √(2/fin) . or μ = 0 and σ2 = sqrt(2/fin) | . | . Remember . use Xavier for Sigmoid, tanh and Softmax | use He for ReLU and Leaky ReLU | . Resources . James Dellinger’s blog post | How to initialize deep neural networks? Xavier and Kaiming initialization | Daniel Godoy’s blog post | Keras initializers | Pytorch forums discussion | Krish Naik’s video | .",
            "url": "https://jithendray.github.io/blogg/2021/02/01/initialization.html",
            "relUrl": "/2021/02/01/initialization.html",
            "date": " • Feb 1, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Breaking down - ARIMA",
            "content": "In a lot of machine learning problems the thing we want to predict is dependent on very clear inputs, such as properties of the target, pixels of an image, etc. In time series these indepent variables are often not known or does not exist. . For example, in predicting the demand of fresh vegetable products from a market, we don’t have a clear independent set of variables where we can fit a model on. A market collects vegetables from various farmers of different places. Is demand of a vegtable product dependent on the properties of a farm it was grown in, or the temperature at that place or the height and weight of the farmer? No, the demand of a vegetable does not depend on the farm or the farmer. The independent data is not easily available or even if we can try to find a relationship between such independent variables and the vegetable demand, these relationships are not perfect and clear. . The time series analysis is frequently used in such cases. The fundamental intuition behind time series forecasting is that, the measure of some variable at a time period will depend on the measure of the same variable at previous time periods. Therefore, we analyze the series on the series itself. . . One of the most used models when handling time series are ARIMA models. ARIMA stands for Autoregressive Integrated Moving Average. I will walk through all the parts of ARIMA to fully explain them. . AR: Auto Regressive model . Auto Regressive (AR) model is a specific type of regression model where, the dependent variable depends on past values of itself. . For example, consider a stock price data. The stock price of today is influenced by yesterday’s price. If the price(close price) of day t-1 is x dollars, the price of day t starts with x dollars. We can assume the price can be determined by the following model: . Yt = μ + ϕYt-1 + ϵt . where μ and ϕ are constants, and ϵt is white noise. . This model is called AR model, and generally AR(p) is given by the following definition: . Yt = μ + ϵt + ϕ1Yt-1 + ϕ2Yt-2 + …+ ϕpYt-p . MA: Moving Average model . Moving Average (MA) model works by analysing how wrong you were in predicting values for the previous time-periods to make a better estimate for the current time-period. This model factors in errors from the observations. . Generally MA(q) is given by the following definition: . Yt = μ + ϵt + θ1 ϵt-1 + θ2 ϵt-2 + …+ θq ϵt-q . MA model complements the AR model by taking the errors from the previous time-periods into considerations, to get better forecast results. . The combined model between AR and MA is called ARMA model, and it is defined as AR(p,q). It is given as follows: . Yt = μ + ϵt + ϕ1Yt-1 + …+ ϕpYt-p + θ1 ϵt-1 + …+ θq ϵt-q . Estimating the ARMA(p,q) model hyperparameters . When selecting the parameters p and q, we must focus on characteristics of the model. Each model have different characteristics for autocorrelation function (ACF) and partial autocorrelation function (PACF). By looking at the ACF and PACF plots, we can identify the numbers of MA and AR terms i.e. q and p respectively. . Autocorrelation . The relationship between two variables is summarized by correlation. . If the correlation is calculated for the timeseries observations with the observations at previous time steps is called as Autocorrelation. The observations of previous timesteps are called as lags. The ACF plot is a bar chart of the coefficients of correlation between timeseries and lags of itself. . Partial Autocorrelation . The “partial” correlation between two variables is the amount of correlation between them which is not explained by their mutual correlations with a specified set of other variables. . If the correlation is calculated for the timeseries observations with the observations at previous time steps at lag k by eliminating all the effects of shorter lags i.e. 1,2,…k-1, then it is called as Partial Autocorrelation. The PACF plot is a plot of the partial correlation coefficients between the series and lags of itself. . Characteristics of ACF and PACF . Now, the models have the following characteristics for the autocorrelation and partial autocorrelation. . AR(p): When the lag is getting large, The autocorrelation decreases exponentially and the partial autocorrelation cuts-off after lag p. | MA(q): When the lag is getting large, the autocorrelation cuts-off after lag q and the partial autocorelation tails off to zero. | ARMA(p,q): When the lag is getting large, both autocorrelation and partial autocorrelation tails off to zero. | . Using these characteristics, we can estimate the proper model. . Stationarity . AR, MA and ARMA models require the data to be stationary. A stationary series has a constant mean and variance over time. But in real-world, most of the data is not stationary. . So how to forecast non-stationary series? . Well, here comes the ARIMA model which works with data that is not stationary. The new term added for ARIMA is I. . Integrated (I) . Consider a non-stationary series which needs to be forecasted. We can say whether the data is stationary or not by studying the plot against time. . . It is clearly visible that the mean is increasing over time i.e. the series is not stationary. If this upward trend is eliminated, the series becomes stationary. The easiest way to do this is to consoder the differences between consecutive timesteps. It goes as follows: . It = Yt+1 - Yt . After applying this transformation, the series becomes like this with observable linear trend - . . The transformed series is called as the differenced series. This differenced series is used for forecasting instead of the timeseries. This step of converting non-stationary series to stationary series results in the new term I which stands for Integrated. (Note: This has nothing to with integration). . In the above example, the data becomes stationary after performing the first order differencing which means a single time differencing of observations at consecutive timesteps. But in some cases, the data remains non-stationary after performing the first order differencing. Hence, the series could be differenced more than once to make it stationary. . d = 1 : It = Yt+1 - Yt . d = 2 : Jt = It+1 - It = Yt+2 + Yt - 2Yt+1 . Hence, ARIMA has hyperparameters p, q and d: . p - order of the AR model | q - order of the MA model | d - order of differencing | . Seasonality . It is to be noted that ARIMA model assumes that the data is not seasonal. The presence of variations that occur at specific regular intervals, such as weekly or monthly results in seasonality of time series. For example, Arrival of vegetables to the market or sales of Diwali crackers. ARIMA does not work well for seasonal series. . There is an upgrade of ARIMA model, called Seasonal ARIMA or SARIMA. It is represented as ARIMA (p,d,q) X (P,D,Q,m). . p - trend AR order | q - trend MA order | d - trend difference order | P - seasonal AR order | Q - seasonal MA order | D - seasonal difference order | m - frequency of seasonality in timeseries | . To identify values for the seasonal model, ACF and PACF plots can be analyzed by looking at correlation at seasonal lag time steps i.e. the lag at which seasonality occurs. Alternately, this post explains how grid searching can be used across the trend and seasonal hyperparameters. . Another easy way to use is to use pmdarima’s auto.arima which automatically gives the best fit model. . Further Reading . Fundamentals of Time series Data and Analysis | ARIMA models for time series forecasting | Summary of rules for identifying ARIMA models | Notes on nonseasonal ARIMA models | How does auto.arima() work? | . Credits . Thanks for reading! And please feel free to connect to me via twitter or linkedin. Feedback is always welcome. .",
            "url": "https://jithendray.github.io/blogg/2020/11/11/Understanding-ARIMA.html",
            "relUrl": "/2020/11/11/Understanding-ARIMA.html",
            "date": " • Nov 11, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Time Series  for beginners",
            "content": "If you are walking the path of becoming a data scientist, you might have already come across the term Time Series and you might have also realized the importance of Time Series Analysis and Forecating. In this post, I will try to give a gentle introduction so that it can kickstart your learning. . . What is Time series? . As the name suggests, time series is just a series of observations collected over different points in time. There exists a correlation between the observations collected at adjacent time points, therefore the previous observations of a variable can be used in predicting the same variable. This distinguishes time series data from general machine learning data where the observtions are collected at a single point in time. . The data collected over time represents a time series only if the observations are dependent on time. If the data collected is purely random in nature, forecating the future values is not possible and such data is called white noise.  . Univariate vs Multivariate time series . If only a single variable is varying over time, it is called Univariate time series. For example, temperature of a room measured every hour. Here there are no other variables recorded, hence predicting temperature only depends on the temperature values recorded at previous time points. . If there are more than one variable varying over time, it is called Multivariate time series. For example, if the humidity is recorded along with the temperature then both temperature and humidity are to be considered in order to predict the temperature. . Note: Predecting the future is not the only goal of time series data. We can have different goals while working with time series. These goals can be mainly categorized into analysis and forecasting. . Time Series Analysis . The main goal of time series analysis is exctracting useful statistics from data in order to understand the nature and underlying causes of the past. It helps to describe available data and provide interpretation to understand the problem domain better. Time series analyis can help to make better predictions. . Time Series Forecasting . The main goal of forecasting is to build models on the past data and use them to predict future observations. For example, predicting number of births in a country based on the data collected in past years. This is challenging as the future observations are unavailable and must be predicted from what has already happened in the past. . Stationarity in Time Series . If all the statistical characteristics of data like mean, auto correlation, variance do not vary with time then the time series is called stationary. But in general, most of the data recorded in not stationary i.e. the properties vary with time. . Analyzing such time series helps to understand the patterns such as trend, seasonality,cyclicality and irregularity. Trend is a general direction the data is changing as time passes. Seasonality is when a pattern recurs over fixed regular time intervals. Cyclicality is when there are any fluctuations around the trend. Unlike seasonality, cyclicality may vary in length. Irregularity is when there are random fluctuations which are not systematic and are irregular. These fluctuations cannot be controlled. These are called as time series components. . . Most of the forecasting methods assume that the data is stationary because it is easy to predict the stationary data. Therfore, it is important to convert non-stationary data to stationary in order to apply forecasting models. . Check out this post by Mehul Gupta which explains Why time series has to be stationary more clearly. . It is important to analyze these components carefully in order to better understand the problem during analysis or forecasting. Since it is difficult to see all the components in a time series, a method called Decomposition can be used to identify them. These components can either combine in an additive way or in a multiplicative way. . An Additive time series is when the fluctuations in the data do not vary over time. Additive model is linear and seasonality has same frequency although the time increases. . Time series = trend + seasonality + cyclicality + irreguarity . A Multiplicative time series is when the variations or the fluctuations in the data increases as the time increases. Multiplicative model is non-linear and seasonality has either increasing or decreasing frequency. . Time series = trend * seasonality * cyclicality * irregularity . Time Series Decomposition . The purpose of decomposition is to identify and seperate components from a time series in order to perform better analysis and forecasting. . In general, the cyclical component is hard to seperate and it is left by grouping it with the trend component, to form a trend-cycle component. It is often simply referred to as the trend component, even though it may contain cyclical behavior. . Classical decomposition can be either a multiplicative or an additive decomposition. A function called seasonal_decompose() can be used to perform classical decomposition. You need to mention whether the model is additive or multiplicative. . Below is an example which shows decomposition of a dataset into components including the original data, trend, seasonality and irregularity(residual). . Let’s first load the dataset and plot a simple graph: . import pandas as pd import matplotlib.pyplot as plt df = pd.read_csv(&#39;births.csv&#39;) df.plot() plt.show() . . Since the variations are very complex, we cannot see all the components clearly. Now, Decomposing this will give us clear picture of the components. Let’s look how we can decompose this using seasonal_decompose() function: . from statsmodels.tsa.seasonal import seasonal_decompose components = seasonal_decompose(df[&#39;births&#39;], model=&#39;multiplicative&#39;,period=10) components.plot() pyplot.show() . . Now you can see all the components, you can analyze them and remove any of them them if not needed. For example, if you want to analyze the trend of a stock data, you would need to remove the seasonality found in the data and the noise due to irregularity. . Let’s look into the basic steps to be followed while performing a forecasting task - . Basic steps for Forecasting . Defining the problem: Understanding the problem domain and clearly knowing the end goal of the forecast. The most important skill needed for a data scientist is being able to explain why a prediction is made and present results in a proper way. This is possible only with having a clear knowledge of who needs the forecast, why and how it will be used. | Data Collection: Collecting the past data related to the problem domain, gathering other important information from domain experts. | Data preperation: This includes exploring the data to know components like trend or seasonality, cleaning the data to fill the missing values and remove outliers if any, basic feature engineering to understand the relation ship between features or to add any new features, resampling and data transforms to remove noise and improve the forecasting. | Modeling: This includes configuring the right forecast model for the data. Widely used time series models are Auto Regressive(AR) models, Moving Average(MA) models, Intergrated(I) models and the combination of these models like Auto Regressive Moving Average models(ARMA), Auto Regressive Integrated Moving Average models(ARIMA). It is better to try models of different types, from simple to advanced approaches. | Evaluation: The time series forecasting model can only be trusted through its performance at predicting the future. This may include testing the model on previous data by creating train-test splits and calculating error or wait for the new observations to occur to compare the predictions. | Applications of Time Series Forecasting . Forecasting of agricultural commodity price | Stock market analysis and forecasting | Sales forecasting | Forecasting supply chain components | Weather forecasting | Forecasting the birth rate in a country | . any many more ….. . Conclusion . This is a basic introduction to time series for beginners. In this post, I’ve explained . What a time series is and why they are important. | Components in a time series data. | Decomposition of time series into it’s components. | Basic steps to be followed while performing a forecasting task. | . The data for plotting the graphs in this post is taken from here. . . Thanks for reading! Please feel free to reach me via twitter. .",
            "url": "https://jithendray.github.io/blogg/timeseries/2020/09/14/Time-Series-Introduction.html",
            "relUrl": "/timeseries/2020/09/14/Time-Series-Introduction.html",
            "date": " • Sep 14, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Understanding Generative Adversarial Networks",
            "content": "When you create something out of nothing, it is the most thrilling thing — Frankie Knuckles, American DJ . Generative Adversarial Networks are exactly the same thrilling thing. They create some new data out of nothing, following the rules established by existing data. This ability to generate information out of nothing makes GANs look like a bit magical. And the results that are generated are very promising. Yann LeCun — one of the most prominent researchers in deep learning described it as “ the most interesting idea in the last 10 years in Machine Learning”. And indeed GANs have had a huge success and thousands of research papers were published in the recent years. . . The main motto of GAN is to generate some information from scratch. But their potential is very huge. Just to give you the idea of their potential, I am mentioning some of the coolest projects created with GANs that you should definitely check out: . . . . So what are Generative Adversarial Networks? What is so magical about them? In this blog post we’ll explore GANs and detailed explanation of how GANs work. But before diving into GANs, we will start by describing what are Generative models. . What is a Generative model? . To understand what a generative model is, contrasting it with a discriminative model is helpful. Discriminative model discriminates between different kinds of data instances whereas a Generative model generates a new data instance. Given the features of a data instance, discriminative model predicts a category to which that data belongs whereas a generative model do the opposite. Instead of predicting a label based on features, a generative model predicts features based on labels. It cares about the distribution of the training data. A Generative Model is a unsupervised learning method that learns any kind of data distribution and it has achieved huge success in the past few years. . Types of Generative models: . . Generative models are of two types — explicit density models and implicit density models. The main difference between them is that explicit models use an explicit density function whereas the implicit models use a stochastic procedure that can directly generate data. . Explicit density models . Explicit density models are again divided into Tractable density and Approximate density models. In general, tractable distribution means it takes polynomial-time to capture probability of its distribution at any given point. Pixel RNN is the most commonly used tractable density model. They are highly effective but they follow sequential generation, which is very slow. . But most of the distributions are complex and it is very difficult to capture the distribution in polynomial time. Such models are considered as Approximate models. These are again divided into two categories: models using variational methods and models using Monte Carlo methods. Variational methods use deterministic approximations and are used in complex models with unknown parameters. Variational Autoencoder (VAE) is one of the most popular generative models and is based on variational learning. Boltzmann machines are another kind of generative models that rely on Markov chains. They use stochastic approximations instead of deterministic approach. Boltzmann machines played an important part in deep learning research but now they are very rarely used because Markov chains inflict very high computational costs. . Implicit density models . These can be trained without explicitly defining a density function. There are some implicit models too which rely on Markov chains like Generative Stochastic Network (GSN) but as we already discussed that Markov chains inflict high computational costs, they fail in many cases. Generative Adversarial Networks were designed to avoid most of these disadvantages associated with other generative models. GANs have become so popular because they are proven to be really successful in modeling and generating high dimensional data. . Enough of this background knowledge. Though it is a very captivating field to explore and discuss, I’ll try to leave the in-depth explanation later in another post, we are here for GANs! Now without wasting any further time, let’s understand how do GANs work !!! . Understanding a GAN . The end goal of a generative model is to predict features given a label, instead of predicting a label given features. They try to learn the model to generate the input distribution as realistic as possible. So the main focus here is to design an architecture of a network that takes a simple N dimensional uniform random variable as input and returns another N dimensional random variable that should somehow follow the probability distribution of input sample as output. Now we need to optimize the network through training. In general, the generated random distribution is directly compared to the input sample and use back propagation to lower the distance between true and generated samples. . Ian Goodfellow and his colleagues came up with a brilliant idea in the 2014 paper titled “Generative Adversarial Nets”. They proposed a new framework in which two neural networks compete with each other for estimating generative models. In the following sections we will understand the training process and the math behind GANs. . The GAN architecture: . . A generative neural network is composed of two models: the Generator which generates data from some random uniform distribution and the Discriminator which identifies the fake data from the real data . The output of generator(fake data) is connected to the discriminator input. . The simplest way to understand the architecture is — a generator network trained to generate samples as realistic as possible via adversarial training by introducing a discriminator network, which plays a role of detecting whether the given sample is real or fake. The generator should learn to fool the discriminator into believing that the input sent by generator is real. While the discriminator tries not to get fooled by generator identifying that the data generated is fake. These two models improves their knowledge by competing with each other, until generator wins in fooling the discriminator. . Since we got an overview of GAN architecture, we will now understand how these models compete with each other technically! . Training a GAN: . We define a neural network G(z, θg) that maps random noise variables z to some data x. We also define another neural network D(x, θd) that outputs a single scalar D(x) that represents the probability that the input came from the real dataset. θg and θd represents parameters that define respective neural networks. The generator and the discriminator have two separate training processes. . The discriminator is simply a classifier. We can use any architecture for the discriminator that is appropriate to the type of data we are using. The discriminator is trained in such a way that it classifies the input data as either real or fake. So the parameters (θd) of the discriminator are updated in order to minimize the probability that any fake data sample G(z) is classified as a real one and also to maximize the probability any real data sample x is classified as belonging to the real dataset. In order words, the loss function of discriminator minimizes D(G(z)) and maximizes D(x). Minimizing log(D(G(z))) is same as maximizing log(1-D(G(z))). So the objective for the discriminator becomes: . . The generator learns to make the discriminator classify the data generated as real through feedback from the discriminator. The parameters (θg) of the generator are updated in order to maximize the probability that any fake data sample is classified as a real one. So the loss function of generator maximizes D(G(z)). . . As Ian Goodfellow said, it is essentially two-player minimax game played by generator(G) and discriminator(D). The value function V(G, D) is given by: . . where — . D(x) : probability that the real data instance x is real . G(z) : generator’s output for noise z . D(G(z)) : probability that the generated instance is real . 1-D(G(z)) : probability that the generated instance is fake . A gradient-based optimization algorithm can be used to train the GAN since both the models are neural networks. We perform back propagation which allows the discriminator and generator to improve over time. Based on the classification done by the discriminator we will either have positive or negative feedback in the form of loss gradients. We keep the parameters of generator constant and train the discriminator during which it has to learn how to slap the generator’s flaws. Then we switch the models. We keep the parameters of the discriminator constant and train the generator. . In this way, we train both the networks alternatively and the networks will compete with each other to improve themselves. Eventually the generator generates realistic data and the discriminator will be unable to find the difference between the real data sample and the generated data sample. . I am adding a screenshot from the paper which explains the algorithm on how to train a GAN using stochastic gradient descent. . . The training steps for a GAN can be described like this: . From a random distribution we take some noise and send it to the generator G which produces some generated fake data. | Along with the generated data, we also send the sample of real data to the discriminator D. | The discriminator calculates the loss for both the real and fake data samples and the generator also calculates the loss from the noise. | The two calculated losses are back propagated to their respective networks and the networks learn to improve from these losses. | Apply optimization algorithm like gradient descent and Repeat the whole process. | . “Talk is cheap, Show me the code” — Linus Torvalds . Okay, We’ll now implement a GAN to understand this better. . Implementing a GAN . We are going to implement a GAN using PyTorch. We’ll start by creating a notebook and importing the following dependencies. . import torch import torch.optim as opt import torch.nn as nn import torch.nn.functional as F import torchvision import torchvision.transforms as transforms import numpy as np . Dataset . The Dataset we will be using is the classic MNIST dataset created by LeCunn. The dataset consists of 60,000 images of handwritten digits, each with size 28x28 . transform = transforms.ToTensor() # load data trainData = torchvision.datasets.MNIST(&#39;./data/&#39;, download=True, transform=transform, train=True) # creating a loader with data - which helps to iterate over the data batch_size = 64 trainLoader = torch.utils.data.DataLoader(trainData, shuffle=True, batch_size=batch_size . . Discriminator . This network will take an image as its input and return the probability of it belonging to the real dataset or the generated dataset. The input size for each image will be 28x28=784 . The architecture we are going to implement will have three Fully-connected layers, each followed by ReLU non-linearity layer. Since the output should be the probability of the image that says whether it is real or fake, the value should be between (0,1). For this purpose, a Sigmoid function is added to the real-valued output in the last layer. . class Discriminator(nn.Module): def __init__(self): super().__init__() self.model = nn.Sequential( nn.Linear(X_dim, 256), nn.LeakyReLU(inplace=True), nn.Linear(256,128), nn.LeakyReLU(inplace=True), nn.Linear(128, 1), nn.Sigmoid() ) def forward(self, input): return self.model(input) . Generator . The Generator network will take a random noise vector as input and returns a vector of size 784, which resembles a 28x28 image. The last layer will have Tanh activation to clip the image to be [-1,1] — which is same size as the preprocessed MNIST images are bounded. . class Generator(nn.Module): def __init__(self): super().__init__() self.model = nn.Sequential( nn.Linear(n_features, 128), nn.LeakyReLU(inplace=True), nn.Linear(128, 256), nn.LeakyReLU(inplace=True), nn.Linear(256, X_dim), nn.Tanh() ) def forward(self, input): return self.model(input) . Optimization . We’ll use Adam as the optimization algorithm with the learning rate 2e-4 which is not necessarily the optimal value, but selected after various tests. . g_opt = opt.Adam(G.parameters(), lr=2e-4) d_opt = opt.Adam(D.parameters(), lr=2e-4) . Training: . In the above section, we’ve already seen the steps that must be followed to train a GAN. First, we need to calculate losses of both generator and discriminator networks and then back-propagate them. . Discriminator loss function: . . Generator loss function: . . We will be using Binary Cross Entropy Loss or log loss because it resembles both the generator and the discriminator losses. . . For training discriminator, if we replace ŷi with D(x) and yi = 1 we will get the real image loss and if we replace ŷi with D(G(z)) and yi = 0 we will get fake image loss. We will add this together to get the total discriminator loss. . For training generator, we need to minimize log(1 - D(G(z))) which is same as maximizing log(D(G(z))). If we replace ŷi with D(G(z)) and yi = 1, we will get the loss to be maximized. But the problem with most of the frameworks like PyTorch is — they minimize the functions. Since, BCE-loss definition has a minus-sign, this won’t cause us any problem. . We will also create the real-image targets as ones, and the fake-image targets as zeros with shape (batch_size, 1). These will be help us in calculating the losses of generator and discriminator. . Training loop: . for epoch in range(20): G_loss_update = 0.0 D_loss_update = 0.0 for i, data in enumerate(trainLoader): X, _ = data X = X.view(X.size(0), -1) batch_size = X.size(0) # tensor containing ones representing real data target real_target = torch.ones(batch_size, 1) # tensor containing zeroes representing generated data target generated_target = torch.zeros(batch_size, 1) z = torch.randn(batch_size, n_features) # 1. Train discriminator # on real data D_real = D(X) # calculating real data error D_real_loss = F.binary_cross_entropy(D_real, real_target) # on generated data D_generated = D(G(z)) # calculating generated data error D_generated_loss = F.binary_cross_entropy(D_generated, generated_target) # Total discriminator loss D_loss = D_real_loss + D_generated_loss # reset gradients d_opt.zero_grad() # backpropagate D_loss.backward() # update weights with gradients d_opt.step() # 2. Train generator # sample noise and generate some fake data z = torch.randn(batch_size, n_features) D_generated = D(G(z)) # calculating error G_loss = F.binary_cross_entropy(D_generated, real_target) # resetting gradients, backpropagating and updating weights g_opt.zero_grad() G_loss.backward() g_opt.step() G_loss_update = G_loss_update + G_loss.item() D_loss_update = D_loss_update + D_loss.item() print(&#39;Epoch:{}, G_loss:{}, D_loss:{}&#39;.format(epoch, G_loss_update/(i+1), D_loss_update/(i+1))) samples = G(z).detach() samples = samples.view(samples.size(0), 1, 28, 28).cpu() imshow(samples) . We have successfully implemented a GAN. Now, let’s look at the results - . Before training: . . During training at 10th epoch: . . Finally: . . You can check out the complete implementation and run it online — kaggle notebook . . References: . [1] Goodfellow, Ian, et al. “Generative Adversarial Networks” NIPS, 2014. . [2] Uddin, S. M. Nadim. (2019). Intuitive Approach to Understand the Mathematics Behind GAN. 10.13140/RG.2.2.12650.36805. . [3] Ian Goodfellow’s NIPS 2016 tutorial on YouTube. .",
            "url": "https://jithendray.github.io/blogg/deeplearning/2020/07/11/Understanding-Generative-Adversarial-Networks.html",
            "relUrl": "/deeplearning/2020/07/11/Understanding-Generative-Adversarial-Networks.html",
            "date": " • Jul 11, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Upgrading Ubuntu to 20.04 LTS",
            "content": "I am very excited about the new version of Ubuntu. The beta version was released a month ago. But since it was fairly stable, I didn’t installed it. I killed my excitement and waited for the LTS version. And finally, on April 23rd Ubuntu 20.04 LTS was released. In this post, I’m going to share the process I used to upgrade my Ubuntu from 19.10 to 20.04. . Note: This is only when you are already running some ubuntu version and want to upgrade it to the new version. This is not the process for fresh installation of Ubuntu. . If you want to upgrade the Ubuntu version, you do need to download any .iso file or boot any USB drive. All you need is: . A good internet connection | And a bit of patience | . Note: You can upgrade from either Ubuntu 18.04 or Ubuntu 19.10 to 20.04LTS. But, If you are running the 16.04 version you will need to upgrade to 18.04 first and then to 20.04 LTS. . Another Note: Backup of important data is always required while performing a major upgrade. (But, I forgot to backup my data and luckily all the data is safe.) . This can be done using Ubuntu’s built-in do release upgrade tool which is also an easy way to upgrade. But I upgraded manually by updating the sources list with the guidance of Nerd on the street. The reason for upgrading manually is simply to enjoy the essence of linux. . Alright, here are the steps I followed to upgrade - . Step 1: Open the terminal and type the following commands . jithendra@hp:~$ cd /etc/apt . jithendra@hp:/etc/apt$ ls . The sources.list file tells ubuntu to what repositories to check in addition to any files in sources.list.d that end in the former one. In order to manually update, both of these should be updated. I started with sources.list . Step 2: Update ‘sources.list’ . jithendra@hp:/etc/apt$ sudo gedit sources.list . Step 3: On active lines, replace the codename of previous Ubuntu version with codename of Ubuntu 20.04 . The codename for Ubuntu 19.10 is eano and for Ubuntu 18.10, it is bionic. | Go through all active lines and replace these with focal. | Note: Active lines are the lines that are not starting with ‘#’. | . | The codename for Ubuntu 20.04 is Focal fossa and the keyword is focal. | Since I was using Ubuntu 19.10, I changed the keyword from eano to focal. | If the version is 18.04, then change from bionic to focal. | If the version is 19.04, then change it from disco to focal. | Save the file | . After updating sources.list file, I went back and check sources.list.d to see any 3rd party apps and ppa’s that also need to be upgraded. For example, google-chrome, spotify, and ppa’s that I might have installed. But, I dont want any third party applications and ppa’s in the new version. So I simply removed all the existing ppas from source.list.d using this command . sudo rm /etc/apt/sources.list.d/* . But, If you want your ppa’s and apps do the following . Step 4: Come back and verify ‘sources.list.d’ . jithendra@hp:/etc/apt$ cd sources.list.d/ . jithendra@hp:/etc/apt/sources.list.d$ ls . This outputs files like google-chrome.list, spotify.list, or ppa’s. | Third party apps dont use any codename in their repository. But this is not case for all third party repositories like ppa’s(personal package archives) hosted on launchpad. | If the repositories uses codenames, then gedit them manually in the same way I did for sources.list | Google-chrome, Spotify and many other third-party applications use stable codename in their repositories. There is no need in updating these codenames. Even some apps use xenial in codenames which need not be updated. | . Step 5: Update and Upgrade . jithendra@hp:/etc/apt/sources.list.d$ cd . jithendra@hp:~$ sudo apt update . I got a whole lot of stuff and all the gits of the updates are pointed out to focal. | This is just telling apt manually to look into focal and not into eano.(in my case) | In my case, I got like this 1873 packages can be upgraded. Run ‘apt list –upgradable’ to see them.` | This means 1873 packages are upgradable which are new. | . jithendra@hp:~$ sudo apt dist-upgrade . or . jithendra@hp:~$ sudo apt full-upgrade . There will be four kinds of packages in here The following packages were automatically installed and are no longer required (Use ‘sudo apt autoremove’ to remove them) | The following packages will be REMOVED | The following NEW packages will be installed | The following packages have been kept back | The following packages will be upgraded | . | Press yes and it will download all the packages and they will be unpacked one by one. | A question will be asked whether to restart servieces during package upgrades without asking - Select YES | Another question will be asked after sometime, whether I want to install package maintainer’s version of the configuration file or user currently installed version - select package manager’s version - select Y | ALMOST DONE | . Step 6: Remove the older clutter . jithendra@hp:~$ sudo apt autoremove –purge . purge removes configuration files too | . Step 7: REBOOT when everything is completed . When I start using my laptop then I faced an issue. | Because the new versions are on the harddrive. | To load all of that - | . jithendra@hp:~$ sudo systemctl reboot . Without any errors, My laptop was succesfully upgraded to Ubuntu 20.04 LTS. . Thank you for reading.Hope you find this post helpful. .",
            "url": "https://jithendray.github.io/blogg/tech/2020/05/01/upgrading-ubuntu.html",
            "relUrl": "/tech/2020/05/01/upgrading-ubuntu.html",
            "date": " • May 1, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://jithendray.github.io/blogg/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I’m a final year CS undergrad at Indian Institute of Information Technology, Jabalpur passionate about data science and machine learning research. My research interests include Machine Learning and Time series forecasting. . Currently, I’m interning as a Data Scientist @ NeenOpal, which is a global management consulting firm with a unique and specialized focus on Data Science based out in Banglore. . In the past, I’ve taken an interest in high performance deep learning on FPGA’s and statistical modeling for time series. During my undergraduation I designed a Neural Network accelerator on Intel CycloneV SoC FPGA using High Level Synthesis tools and languages. I also worked on forecasting project under Prof. Sunil Agrawal where I forecasted Onion arrival to Mumbai (a city in India) markets from farmers. . In the summer of 2019, I was a visiting researcher at HCI Lab, IIIT Allahabad working on early detection of Autism in toddlers using Machine Learning and Computer Vision. I was fortunate enough to work under the amazing mentorship of Prof. Anupam Agrawal. . I actively publish code to GitHub. I’m also active on Twitter and LinkedIn. If you think me (or my projects) are interesting feel free to contact me! You can also mail me at jithendra1230@gmail.com. I’m always interested in hearing about new opportunities to collaborate and work on machine learning projects or hackathons. . I spend my time analysing some publicly available data, reading research papers and writing. I spend my liesure time mostly banging my head to some heavy metal and some times I read books. Unfortunately, 2020 compelled me to blog about my weird music taste. I started writing my personal thoughts and impressions on Metal bands and their albums on Medium. . “Writing isn’t about making money, getting famous, getting dates, getting laid, or making friends. In the end it’s about enriching the lives of those who will read your work, and enriching your own life as well. It’s about getting up, getting well, and getting over. Getting happy, okay?” – Stephen King, On Writing: A Memoir of the Craft. . Links: . Twitter | LinkedIn | GitHub | .",
          "url": "https://jithendray.github.io/blogg/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://jithendray.github.io/blogg/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}