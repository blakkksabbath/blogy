<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Weight Initialization | Jithendra’s blog</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Weight Initialization" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="My notes on initializing weights" />
<meta property="og:description" content="My notes on initializing weights" />
<link rel="canonical" href="https://jithendrabsy.github.io/blogy/machinelearning/deeplearning/2021/02/01/initialization.html" />
<meta property="og:url" content="https://jithendrabsy.github.io/blogy/machinelearning/deeplearning/2021/02/01/initialization.html" />
<meta property="og:site_name" content="Jithendra’s blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-02-01T00:00:00-06:00" />
<script type="application/ld+json">
{"url":"https://jithendrabsy.github.io/blogy/machinelearning/deeplearning/2021/02/01/initialization.html","@type":"BlogPosting","headline":"Weight Initialization","dateModified":"2021-02-01T00:00:00-06:00","datePublished":"2021-02-01T00:00:00-06:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://jithendrabsy.github.io/blogy/machinelearning/deeplearning/2021/02/01/initialization.html"},"description":"My notes on initializing weights","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blogy/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://jithendrabsy.github.io/blogy/feed.xml" title="Jithendra's blog" /><link rel="shortcut icon" type="image/x-icon" href="/blogy/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/blogy/">Jithendra&#39;s blog</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blogy/about/">About Me</a><a class="page-link" href="/blogy/search/">Search</a><a class="page-link" href="/blogy/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Weight Initialization</h1><p class="page-description">My notes on initializing weights</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-02-01T00:00:00-06:00" itemprop="datePublished">
        Feb 1, 2021
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      2 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/blogy/categories/#machinelearning">machinelearning</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blogy/categories/#deeplearning">deeplearning</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>In this article, I am writing some notes about weight initialization. 
I took these from various sources I am reading regarding this. Links and References at the end of the post! Consider reading them to get a clear understanding!</p>

<p>NOTE: This is post for my future self to look back and review the material. So, this’ll be very unpolished!</p>

<h3 id="introduction">Introduction</h3>
<p>The first step that comes in consideration while building a neural network is the initialization of parameters - weights and biases. 
If not done correctly then layer activations might explode or vanish during the forward propagation which in turn makes loss gradients to be either too large or too small.<br />
Then achieving optimization will take longer or sometimes converging to a minima using gradient descent will be impossible.</p>

<h3 id="some-key-points-to-remember">Some key points to remember</h3>
<ul>
  <li>If the weights are initialized too large or too small, the network won’t learn well - because it leads to exploding or vanishing gradients problem.</li>
  <li>All weights should not be initialized with zeros.
    <ul>
      <li>If neurons starts with same weights, then all neurons will learn the same features and perform the same thing as one another.</li>
      <li>Neural Networks try to  reach the local minima, If all the weights start at zero - it is not possible. So, it is better to give them different starting values.</li>
    </ul>
  </li>
</ul>

<h3 id="weight-initialization-methods">Weight Initialization methods</h3>

<h4 id="normal-initialization">Normal Initialization</h4>
<p>The authors of the famous <a href="https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html">Alexnet Paper</a> initialized weights using 
zero-mean Guassian (normal) distribution with a standard deviation of 0.01. The biases were initialized as 1 for some layers and 0 for the rest.</p>

<p><strong>Uniform initialization</strong>: bounded uniformly between ~ [$\frac{-1}{\sqrt{f_{in}}}, \frac{1}{\sqrt{f_{in}}}$]</p>

<p>But this normal random initialization of weights does not work well for training deep neural networks, because of vanishing and exploding gradient problem.</p>

<h4 id="xavier-initialization--glorot-initialization-paper">Xavier Initialization / Glorot initialization [<a href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf?hc_location=ufi]">paper</a>]</h4>

<ul>
  <li>Proposed by Xavier and Bengio</li>
  <li>considers number of input and output units while initializing weights</li>
  <li>weights stay within a reasonable range by making them inversely proportional to the square root of the number of units in the previous layer</li>
</ul>

<p><strong>Uniform</strong>: bounded uniformly between ~ [$\pm \sqrt { \frac {6} {f_{in} + f_{out}}}$]</p>

<p><strong>Normal</strong>: multiply normal distribution by $\sqrt { \frac {2} {f_{in} + f_{out}}}$</p>

<ul>
  <li>np.random.rand(shape) * np.sqrt( $\frac {2} {f_{in} + f_{out}}$)</li>
  <li>or create normal distribution with $\mu$ = 0 and $\sigma^2$ = $\sqrt { \frac {2} {f_{in} + f_{out}}}$</li>
</ul>

<h4 id="he-initialization--kaiming-initialization-paper">He initialization / Kaiming initialization <a href="https://arxiv.org/abs/1502.01852">paper</a></h4>
<ul>
  <li>RELU activations are mostly used - bercause they are robust to vanishing/ exploding gradients.</li>
  <li>A more robust initialization technique was introduced by Kaiming et al.  for activation functions like RELU.</li>
  <li>both Xavier and He use similar theory →
    <ul>
      <li>find a good variance for the distribution from which the initial parameters are drawn</li>
      <li>This variance is adapted to the activation function used</li>
      <li>
        <p>derived without explicitly considering the type of the distribution</p>

        <p><img src="/blogy/images/posts/2021-02-01/int.png" alt="" /></p>
      </li>
      <li>Red → He and Blue → Xavier</li>
    </ul>
  </li>
</ul>

<p><strong>Uniform</strong>: [$\pm \sqrt {\frac {6} {f_{in}} }$]</p>

<p><strong>Normal</strong>: normal distribution * $\sqrt {\frac {6} {f_{in}}}$</p>

<ul>
  <li>or $\mu$ = 0 and $\sigma^2$ = $\sqrt{\frac{2}{f_{in}}}</li>
</ul>

<h3 id="remember">Remember</h3>
<ul>
  <li>use Xavier for Sigmoid, tanh and Softmax</li>
  <li>use He for ReLU and Leaky ReLU</li>
</ul>

<h3 id="resources">Resources</h3>
<ul>
  <li><a href="https://towardsdatascience.com/weight-initialization-in-neural-networks-a-journey-from-the-basics-to-kaiming-954fb9b47c79">James Dellinger’s blog post</a></li>
  <li><a href="https://pouannes.github.io/blog/initialization/">How to initialize deep neural networks? Xavier and Kaiming initialization</a></li>
  <li><a href="https://towardsdatascience.com/hyper-parameters-in-action-part-ii-weight-initializers-35aee1a28404">Daniel Godoy’s blog post</a></li>
  <li><a href="https://keras.io/api/layers/initializers/">Keras initializers</a></li>
  <li><a href="https://discuss.pytorch.org/t/whats-the-default-initialization-methods-for-layers/3157/2">Pytorch forums discussion</a></li>
  <li><a href="https://www.youtube.com/watch?v=tMjdQLylyGI">Krish Naik’s video</a></li>
</ul>

  </div><a class="u-url" href="/blogy/machinelearning/deeplearning/2021/02/01/initialization.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blogy/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/blogy/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/blogy/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>A place where I scribble down my thoughts and notes for my future self to look back and review the material.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/jithendrabsy" title="jithendrabsy"><svg class="svg-icon grey"><use xlink:href="/blogy/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://www.linkedin.com/in/jithendrabsy" title="jithendrabsy"><svg class="svg-icon grey"><use xlink:href="/blogy/assets/minima-social-icons.svg#linkedin"></use></svg></a></li><li><a rel="me" href="https://twitter.com/jithendrabsy" title="jithendrabsy"><svg class="svg-icon grey"><use xlink:href="/blogy/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
